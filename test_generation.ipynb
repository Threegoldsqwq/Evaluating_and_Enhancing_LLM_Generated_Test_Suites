{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8b8c13de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pathlib\n",
    "from datasets import load_dataset\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0f189148",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gemini-2.5-flash\"\n",
    "OUT_DIR = pathlib.Path(\"generated_tests\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ae0b1e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_docstring(prompt_text: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract all triple-quoted docstrings from the input text.\n",
    "    Returns a list of docstring contents as strings.\n",
    "    \"\"\"\n",
    "    matches = re.findall(r'(\"\"\"|\\'\\'\\')(.*?)(\\1)', prompt_text, flags=re.DOTALL)\n",
    "    # Join all docstrings into a single string (if needed)\n",
    "    doc_text = \"\\n\\n\".join([m[1].strip() for m in matches])\n",
    "\n",
    "    return doc_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c6184636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HumanEval with 164 problems.\n"
     ]
    }
   ],
   "source": [
    "api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"Please set GOOGLE_API_KEY in your environment.\")\n",
    "genai.configure(api_key=api_key)\n",
    "model = genai.GenerativeModel(MODEL)\n",
    "\n",
    "ds = load_dataset(\"openai/openai_humaneval\")[\"test\"]  # 164 items\n",
    "print(f\"Loaded HumanEval with {len(ds)} problems.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d6520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, item in enumerate(ds):\n",
    "    doc = extract_docstring(item[\"prompt\"])\n",
    "    prompt = f'\"\"\"\\n{doc}\\n\"\"\"\\nPlease generate 10 test cases in Python\\'s standard unittest format for this problem. Please ONLY generate test cases, assume the function exist.'\n",
    "    if(idx == 32):\n",
    "        print(prompt)\n",
    "    # try:\n",
    "    #     resp = model.generate_content(prompt)\n",
    "    #     text = (resp.text or \"\").strip()\n",
    "    #     # If fences sneak in, strip them.\n",
    "    #     text = re.sub(r\"^```(?:python)?\\s*\", \"\", text)\n",
    "    #     text = re.sub(r\"\\s*```$\", \"\", text)\n",
    "\n",
    "    #     out_path = OUT_DIR / f\"HumanEval_{idx}.py\"\n",
    "    #     out_path.write_text(text, encoding=\"utf-8\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"[WARN] Problem {idx} failed: {e}\")\n",
    "        \n",
    "print(\"Generation Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7401322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 164 canonical solutions...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pathlib\n",
    "import textwrap\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# === CONFIG ===\n",
    "OUT_DIR = pathlib.Path(\"canonical_solutions\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "INCLUDE_IMPORTS = True  # set False to drop import lines and keep only `def ...:` + body\n",
    "\n",
    "def extract_signature_from_prompt(prompt: str, include_imports: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    From HumanEval `prompt`, remove the triple-quoted docstring and 'pass',\n",
    "    then return the remaining code (imports + def line).\n",
    "    \"\"\"\n",
    "    # remove first triple-quoted docstring block (\"\"\"...\"\"\" or '''...''')\n",
    "    code_wo_doc = re.sub(r'(\"\"\"|\\'\\'\\')(.*?)(\\1)', '', prompt, flags=re.DOTALL)\n",
    "    # remove any bare 'pass' lines\n",
    "    code_wo_doc = re.sub(r'^[ \\t]*pass[ \\t]*\\r?\\n?', '', code_wo_doc, flags=re.MULTILINE)\n",
    "    # normalize whitespace\n",
    "    lines = [ln.rstrip() for ln in code_wo_doc.strip().splitlines() if ln.strip()]\n",
    "\n",
    "    if not lines:\n",
    "        raise ValueError(\"No signature content found in prompt.\")\n",
    "\n",
    "    # Find the first def line\n",
    "    def_idx = next((i for i, ln in enumerate(lines) if ln.lstrip().startswith(\"def \")), None)\n",
    "    if def_idx is None:\n",
    "        # fallback: sometimes there's a blank before def; just join everything\n",
    "        joined = \"\\n\".join(lines) + (\"\\n\" if not lines[-1].endswith(\"\\n\") else \"\")\n",
    "        return joined\n",
    "\n",
    "    if include_imports:\n",
    "        kept = lines[:def_idx+1]  # imports (if any) + the def line\n",
    "    else:\n",
    "        kept = [lines[def_idx]]   # only the def line\n",
    "\n",
    "    sig = \"\\n\".join(kept)\n",
    "    if not sig.endswith(\"\\n\"):\n",
    "        sig += \"\\n\"\n",
    "    return sig\n",
    "\n",
    "def assemble_module(signature_code: str, body_code: str) -> str:\n",
    "    \"\"\"\n",
    "    Indent the canonical_solution body under the def line.\n",
    "    Handles cases where body is already (or not) indented.\n",
    "    \"\"\"\n",
    "    body = textwrap.dedent(body_code.rstrip(\"\\n\")) + \"\\n\"\n",
    "    body_indented = textwrap.indent(body, \"    \")\n",
    "    return signature_code + body_indented\n",
    "\n",
    "def get_split(ds_any) -> Dataset:\n",
    "    \"\"\"\n",
    "    Accept either:\n",
    "      - a DatasetDict with key 'test'\n",
    "      - a Dataset that is already the split\n",
    "    \"\"\"\n",
    "    if isinstance(ds_any, DatasetDict):\n",
    "        return ds_any[\"test\"]\n",
    "    if isinstance(ds_any, Dataset):\n",
    "        return ds_any\n",
    "    raise TypeError(\"Provide a Hugging Face Dataset or DatasetDict (with 'test').\")\n",
    "\n",
    "try:\n",
    "    split = get_split(ds)       # if your variable is named ds\n",
    "except NameError:\n",
    "    ds = load_dataset(\"openai/openai_humaneval\")[\"test\"]  # 164 items\n",
    "    split = get_split(ds) \n",
    "\n",
    "print(f\"Writing {len(split)} canonical solutions...\")\n",
    "\n",
    "for i, item in enumerate(split):\n",
    "    task_id = item.get(\"task_id\", f\"HumanEval/{i}\")\n",
    "    entry_point = item.get(\"entry_point\", f\"task_{i}\")\n",
    "    prompt = item[\"prompt\"]\n",
    "    body = item[\"canonical_solution\"]\n",
    "\n",
    "    # 1) signature\n",
    "    signature = extract_signature_from_prompt(prompt, include_imports=INCLUDE_IMPORTS)\n",
    "\n",
    "    # 2) final module\n",
    "    final_code = assemble_module(signature, body)\n",
    "\n",
    "    # 3) filename like 000_entrypoint.py\n",
    "    idx_str = str(i).zfill(3)\n",
    "    out_path = OUT_DIR / f\"{idx_str}_{entry_point}.py\"\n",
    "    out_path.write_text(final_code, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "552c4c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, sys, re, json, subprocess, pathlib, textwrap\n",
    "# import pandas as pd\n",
    "\n",
    "# CANON_DIR = pathlib.Path(\"canonical_solutions\").resolve()\n",
    "# TESTS_DIR = pathlib.Path(\"generated_tests\").resolve()\n",
    "# assert CANON_DIR.is_dir(), f\"Missing {CANON_DIR}\"\n",
    "# assert TESTS_DIR.is_dir(), f\"Missing {TESTS_DIR}\"\n",
    "\n",
    "# # --------------------------------------------------------------------\n",
    "# # 1) Create a single tiny runner (only once)\n",
    "# #    This runner:\n",
    "# #      - reads env vars: CANDIDATE_FILE, TEST_FILE\n",
    "# #      - loads the canonical solution as module \"candidate\"\n",
    "# #      - runs unittest on TEST_FILE\n",
    "# #      - prints a single line of JSON with {\"testsRun\", \"failures\", \"errors\", \"passed\"}\n",
    "# # --------------------------------------------------------------------\n",
    "# runner_path = pathlib.Path(\"_test_runner.py\").resolve()\n",
    "# if not runner_path.exists():\n",
    "#     runner_code = r'''\n",
    "# # _test_runner.py\n",
    "# import os, sys, types, json, unittest, pathlib, builtins, importlib.util\n",
    "\n",
    "# CANDIDATE_FILE = os.environ.get(\"CANDIDATE_FILE\")\n",
    "# TEST_FILE = os.environ.get(\"TEST_FILE\")\n",
    "# ENTRY_POINT = os.environ.get(\"ENTRY_POINT\")\n",
    "\n",
    "# def die(msg, code=2):\n",
    "#     print(json.dumps({\"error\": msg}))\n",
    "#     sys.exit(code)\n",
    "\n",
    "# if not CANDIDATE_FILE or not TEST_FILE:\n",
    "#     die(\"missing env CANDIDATE_FILE/TEST_FILE\")\n",
    "\n",
    "# # 1) Load canonical solution module as \"candidate\"\n",
    "# sol_path = pathlib.Path(CANDIDATE_FILE)\n",
    "# code = sol_path.read_text(encoding=\"utf-8\")\n",
    "# candidate_mod = types.ModuleType(\"candidate\")\n",
    "# exec(compile(code, str(sol_path), \"exec\"), candidate_mod.__dict__)\n",
    "# sys.modules[\"candidate\"] = candidate_mod\n",
    "\n",
    "# # 2) Put symbol in builtins (safety net)\n",
    "# if ENTRY_POINT and hasattr(candidate_mod, ENTRY_POINT):\n",
    "#     setattr(builtins, ENTRY_POINT, getattr(candidate_mod, ENTRY_POINT))\n",
    "\n",
    "# # 3) Load the specific test file as a module so we can inject into its globals\n",
    "# test_path = pathlib.Path(TEST_FILE)\n",
    "# spec = importlib.util.spec_from_file_location(\"test_generated\", str(test_path))\n",
    "# if spec is None or spec.loader is None:\n",
    "#     die(f\"cannot load {TEST_FILE}\")\n",
    "# test_mod = importlib.util.module_from_spec(spec)\n",
    "\n",
    "# # Inject the entry point into the test module globals BEFORE executing it\n",
    "# if ENTRY_POINT and hasattr(candidate_mod, ENTRY_POINT):\n",
    "#     test_mod.__dict__[ENTRY_POINT] = getattr(candidate_mod, ENTRY_POINT)\n",
    "\n",
    "# sys.modules[\"test_generated\"] = test_mod\n",
    "# spec.loader.exec_module(test_mod)  # executes the test file\n",
    "\n",
    "# # 4) Build suite from that module and run\n",
    "# suite = unittest.defaultTestLoader.loadTestsFromModule(test_mod)\n",
    "# result = unittest.TextTestRunner(verbosity=0).run(suite)\n",
    "\n",
    "# out = {\n",
    "#     \"testsRun\": result.testsRun,\n",
    "#     \"failures\": len(result.failures),\n",
    "#     \"errors\": len(result.errors),\n",
    "#     \"passed\": result.testsRun - len(result.failures) - len(result.errors),\n",
    "# }\n",
    "# print(json.dumps(out))\n",
    "# # exit code 0 even on failures; coverage/mutmut decide what they need\n",
    "# '''\n",
    "#     runner_path.write_text(textwrap.dedent(runner_code), encoding=\"utf-8\")\n",
    "#     print(f\"Created {runner_path.name}\")\n",
    "\n",
    "# def run_cmd(cmd, cwd=None, env=None):\n",
    "#     p = subprocess.Popen(cmd, cwd=cwd, env=env,\n",
    "#                          stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n",
    "#                          text=True)\n",
    "#     out, err = p.communicate()\n",
    "#     return p.returncode, out, err\n",
    "\n",
    "# # Build an index from your canonical_solutions files: 000_xxx.py -> (idx, entry_point, path)\n",
    "# canon_index = {}\n",
    "# for f in sorted(CANON_DIR.glob(\"*.py\")):\n",
    "#     m = re.match(r\"^(\\d{3})_(.+)\\.py$\", f.name)\n",
    "#     if not m:\n",
    "#         continue\n",
    "#     idx = int(m.group(1))\n",
    "#     entry = m.group(2)\n",
    "#     canon_index[idx] = (entry, f)\n",
    "\n",
    "# rows = []\n",
    "\n",
    "# for idx, (entry_point, sol_path) in canon_index.items():\n",
    "#     test_file = TESTS_DIR / f\"HumanEval_{idx}.py\"\n",
    "#     if not test_file.exists():\n",
    "#         print(f\"[SKIP] No test for {idx}: {test_file.name}\")\n",
    "#         continue\n",
    "\n",
    "#     # -----------------------\n",
    "#     # A) RUN UNDER COVERAGE\n",
    "#     # -----------------------\n",
    "#     env = os.environ.copy()\n",
    "#     env[\"PYTHONPATH\"] = str(TESTS_DIR) + os.pathsep + env.get(\"PYTHONPATH\", \"\")\n",
    "#     env[\"CANDIDATE_FILE\"] = str(sol_path)\n",
    "#     env[\"TEST_FILE\"] = str(test_file)\n",
    "#     env[\"ENTRY_POINT\"] = entry_point\n",
    "\n",
    "#     # 1) run tests with coverage\n",
    "#     rc, out, err = run_cmd(\n",
    "#         [\"coverage\", \"run\", \"--source\", str(CANON_DIR), str(runner_path)],\n",
    "#         env=env\n",
    "#     )\n",
    "#     # parse stdout JSON from runner for pass counts\n",
    "#     passed = 0\n",
    "#     total = 10\n",
    "#     try:\n",
    "#         last_line = [ln for ln in out.splitlines() if ln.strip()][-1]\n",
    "#         stats = json.loads(last_line)\n",
    "#         passed = int(stats.get(\"passed\", 0))\n",
    "#         total = int(stats.get(\"testsRun\", 10))\n",
    "#     except Exception:\n",
    "#         print(f\"[WARN] could not parse test stats for {idx}. stdout:\\n{out}\\nstderr:\\n{err}\")\n",
    "\n",
    "#     validity_rate = passed / total if total else 0.0\n",
    "\n",
    "#     # 2) get coverage JSON once we have a .coverage file\n",
    "#     cov_json_path = f\".cov_{idx:03d}.json\"\n",
    "#     rc, out_cov, err_cov = run_cmd([\"coverage\", \"json\", \"-o\", cov_json_path], env=env)\n",
    "#     coverage_pct = 0.0\n",
    "#     if rc == 0 and pathlib.Path(cov_json_path).exists():\n",
    "#         cov = json.loads(pathlib.Path(cov_json_path).read_text(encoding=\"utf-8\"))\n",
    "#         files = cov.get(\"files\", {})\n",
    "#         key = str(sol_path)\n",
    "#         if key in files:\n",
    "#             coverage_pct = float(files[key][\"summary\"][\"percent_covered\"])\n",
    "#         else:\n",
    "#             # fallback by basename\n",
    "#             for k, v in files.items():\n",
    "#                 if pathlib.Path(k).name == sol_path.name:\n",
    "#                     coverage_pct = float(v[\"summary\"][\"percent_covered\"])\n",
    "#                     break\n",
    "#         # clean up the per-idx json to keep new files to a minimum\n",
    "#         try:\n",
    "#             pathlib.Path(cov_json_path).unlink()\n",
    "#         except Exception:\n",
    "#             pass\n",
    "#     else:\n",
    "#         print(f\"[WARN] coverage json failed for {idx}: {err_cov or out_cov}\")\n",
    "\n",
    "#     # -----------------------\n",
    "#     # B) MUTATION TESTING\n",
    "#     # -----------------------\n",
    "#     # mutmut will spawn the test runner many times; we provide the same env so\n",
    "#     # _test_runner.py knows which solution and test to use.\n",
    "#     mut_env = env.copy()\n",
    "#     # Important: keep PYTHONPATH so _test_runner.py is importable as a script\n",
    "#     # We'll use the script path, not -m, to avoid path headaches.\n",
    "#     # Limit mutations to THIS single solution file.\n",
    "#     rc, out_run, err_run = run_cmd(\n",
    "#         [\n",
    "#             \"mutmut\", \"run\",\n",
    "#             \"--paths-to-mutate\", str(sol_path),\n",
    "#             \"--runner\", f\"python {runner_path}\",\n",
    "#             \"--tests-dir\", \".\",   # runner ignores this and uses TEST_FILE\n",
    "#             \"--no-progress\",\n",
    "#             \"--use-coverage\"      # speeds up by using coverage cache\n",
    "#         ],\n",
    "#         env=mut_env\n",
    "#     )\n",
    "#     rc, out_res, err_res = run_cmd([\"mutmut\", \"results\"], env=mut_env)\n",
    "\n",
    "#     killed = survived = 0\n",
    "#     for line in (out_res + \"\\n\" + err_res).splitlines():\n",
    "#         m = re.search(r\"\\bKilled\\s*\\((\\d+)\\)\", line)\n",
    "#         if m: killed = int(m.group(1))\n",
    "#         m = re.search(r\"\\bSurvived\\s*\\((\\d+)\\)\", line)\n",
    "#         if m: survived = int(m.group(1))\n",
    "#     total_mutants = killed + survived\n",
    "#     mutation_score = (100.0 * killed / total_mutants) if total_mutants else 0.0\n",
    "\n",
    "#     rows.append({\n",
    "#         \"id\": idx,\n",
    "#         \"entry_point\": entry_point,\n",
    "#         \"passed\": passed,\n",
    "#         \"validity_rate\": validity_rate,\n",
    "#         \"coverage\": coverage_pct,\n",
    "#         \"mutation_score\": mutation_score,\n",
    "#     })\n",
    "#     print(f\"[{idx:03d}] passed={passed}/{total}  cov={coverage_pct:.1f}%  mut={mutation_score:.1f}\")\n",
    "\n",
    "#     if(idx == 10):\n",
    "#         break\n",
    "\n",
    "# # Final table\n",
    "# df = pd.DataFrame(rows).sort_values(\"id\").reset_index(drop=True)\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183de104",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "02fa38d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始运行164个测试...\n",
      "找到 164 个测试文件和 164 个解决方案文件\n",
      "开始运行测试...\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== 总体统计 ===\n",
      "总测试文件: 164\n",
      "总通过测试: 1326/1640\n",
      "总体Validity Rate: 0.8085 (80.85%)\n",
      "平均Validity Rate: 0.8085\n",
      "通过率分布:\n",
      "passed_tests\n",
      "0      26\n",
      "5       1\n",
      "6       2\n",
      "7       1\n",
      "8       9\n",
      "9      21\n",
      "10    103\n",
      "11      1\n",
      "Name: count, dtype: int64\n",
      "==================================================\n",
      "\n",
      "前10行结果:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>test_file</th>\n",
       "      <th>solution_file</th>\n",
       "      <th>passed_tests</th>\n",
       "      <th>total_tests</th>\n",
       "      <th>validity_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>HumanEval_0.py</td>\n",
       "      <td>000_has_close_elements.py</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>HumanEval_1.py</td>\n",
       "      <td>001_separate_paren_groups.py</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>HumanEval_2.py</td>\n",
       "      <td>002_truncate_number.py</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>HumanEval_3.py</td>\n",
       "      <td>003_below_zero.py</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>HumanEval_4.py</td>\n",
       "      <td>004_mean_absolute_deviation.py</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>159</td>\n",
       "      <td>HumanEval_159.py</td>\n",
       "      <td>159_eat.py</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>160</td>\n",
       "      <td>HumanEval_160.py</td>\n",
       "      <td>160_do_algebra.py</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>161</td>\n",
       "      <td>HumanEval_161.py</td>\n",
       "      <td>161_solve.py</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>162</td>\n",
       "      <td>HumanEval_162.py</td>\n",
       "      <td>162_string_to_md5.py</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>163</td>\n",
       "      <td>HumanEval_163.py</td>\n",
       "      <td>163_generate_integers.py</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     test_id         test_file                   solution_file  passed_tests  \\\n",
       "0          0    HumanEval_0.py       000_has_close_elements.py            10   \n",
       "1          1    HumanEval_1.py    001_separate_paren_groups.py             9   \n",
       "2          2    HumanEval_2.py          002_truncate_number.py            10   \n",
       "3          3    HumanEval_3.py               003_below_zero.py            10   \n",
       "4          4    HumanEval_4.py  004_mean_absolute_deviation.py             9   \n",
       "..       ...               ...                             ...           ...   \n",
       "159      159  HumanEval_159.py                      159_eat.py            10   \n",
       "160      160  HumanEval_160.py               160_do_algebra.py             0   \n",
       "161      161  HumanEval_161.py                    161_solve.py            10   \n",
       "162      162  HumanEval_162.py            162_string_to_md5.py             0   \n",
       "163      163  HumanEval_163.py        163_generate_integers.py             7   \n",
       "\n",
       "     total_tests  validity_rate  \n",
       "0             10            1.0  \n",
       "1             10            0.9  \n",
       "2             10            1.0  \n",
       "3             10            1.0  \n",
       "4             10            0.9  \n",
       "..           ...            ...  \n",
       "159           10            1.0  \n",
       "160           10            0.0  \n",
       "161           10            1.0  \n",
       "162           10            0.0  \n",
       "163           10            0.7  \n",
       "\n",
       "[164 rows x 6 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import tempfile\n",
    "import re\n",
    "import glob\n",
    "\n",
    "def extract_test_class_name(test_code):\n",
    "    \"\"\"从测试代码中提取测试类名\"\"\"\n",
    "    class_match = re.search(r'class\\s+(\\w+)\\(unittest\\.TestCase\\)', test_code)\n",
    "    if class_match:\n",
    "        return class_match.group(1)\n",
    "    else:\n",
    "        # 如果无法提取，尝试其他模式\n",
    "        class_match = re.search(r'class\\s+(\\w+)\\s*\\(.*TestCase.*\\)', test_code)\n",
    "        if class_match:\n",
    "            return class_match.group(1)\n",
    "    return None\n",
    "\n",
    "def run_single_test(solution_file, test_file):\n",
    "    \"\"\"运行单个测试并返回通过数量\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # 读取解决方案文件\n",
    "        with open(solution_file, 'r', encoding='utf-8') as f:\n",
    "            solution_code = f.read()\n",
    "        \n",
    "        # 读取测试文件\n",
    "        with open(test_file, 'r', encoding='utf-8') as f:\n",
    "            test_code = f.read()\n",
    "        \n",
    "        # 提取测试类名\n",
    "        test_class_name = extract_test_class_name(test_code)\n",
    "        if not test_class_name:\n",
    "            print(f\"  警告: 无法从 {os.path.basename(test_file)} 中提取测试类名\")\n",
    "            return 0\n",
    "        \n",
    "        # 创建临时文件，合并解决方案和测试代码\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False, encoding='utf-8') as temp_file:\n",
    "            # 写入解决方案代码\n",
    "            temp_file.write(solution_code + \"\\n\\n\")\n",
    "            \n",
    "            # 写入测试代码\n",
    "            temp_file.write(test_code + \"\\n\\n\")\n",
    "            \n",
    "            # 添加测试运行代码\n",
    "            temp_file.write(f\"\"\"\n",
    "import sys\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import unittest\n",
    "    try:\n",
    "        suite = unittest.TestLoader().loadTestsFromTestCase({test_class_name})\n",
    "        runner = unittest.TextTestRunner(verbosity=0, stream=sys.stdout)\n",
    "        result = runner.run(suite)\n",
    "        \n",
    "        # 输出统计信息\n",
    "        print(f\"TESTS_RUN:{{result.testsRun}}\")\n",
    "        print(f\"FAILURES:{{len(result.failures)}}\")\n",
    "        print(f\"ERRORS:{{len(result.errors)}}\")\n",
    "        print(f\"PASSED:{{result.testsRun - len(result.failures) - len(result.errors)}}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR running tests: {{e}}\")\n",
    "        print(\"TESTS_RUN:0\")\n",
    "        print(\"FAILURES:0\")\n",
    "        print(\"ERRORS:1\")\n",
    "        print(\"PASSED:0\")\n",
    "\"\"\")\n",
    "            temp_file_path = temp_file.name\n",
    "        \n",
    "        # 运行测试\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, temp_file_path],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=30  # 30秒超时\n",
    "        )\n",
    "        \n",
    "        # 解析输出\n",
    "        output = result.stdout + result.stderr\n",
    "        \n",
    "        # 从输出中提取通过的数量\n",
    "        passed_match = re.search(r'PASSED:(\\d+)', output)\n",
    "        tests_run_match = re.search(r'TESTS_RUN:(\\d+)', output)\n",
    "        \n",
    "        if passed_match and tests_run_match:\n",
    "            passed_tests = int(passed_match.group(1))\n",
    "            tests_run = int(tests_run_match.group(1))\n",
    "            \n",
    "            return passed_tests\n",
    "        else:\n",
    "            # 如果无法解析输出，使用启发式方法\n",
    "            if \"OK\" in output and (\"FAILED\" not in output or \"FAILURES: 0\" in output):\n",
    "                return 10\n",
    "            else:\n",
    "                # 统计失败和错误\n",
    "                failures = len(re.findall(r'FAIL:', output)) + len(re.findall(r'ERROR:', output))\n",
    "                errors = len(re.findall(r'Traceback', output))\n",
    "                return max(0, 10 - failures - errors)\n",
    "                \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"  超时: {os.path.basename(test_file)}\")\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(f\"  错误运行测试 {os.path.basename(test_file)}: {e}\")\n",
    "        return 0\n",
    "    finally:\n",
    "        # 清理临时文件\n",
    "        try:\n",
    "            if 'temp_file_path' in locals():\n",
    "                os.unlink(temp_file_path)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "def calculate_validity_rate_all_tests():\n",
    "    \"\"\"计算所有164个测试的Validity Rate\"\"\"\n",
    "    \n",
    "    test_dir = \"C:\\\\Users\\\\zhang\\\\Downloads\\\\Evaluating_and_Enhancing_LLM_Generated_Test_Suites\\\\generated_tests\"\n",
    "    solution_dir = \"C:\\\\Users\\\\zhang\\\\Downloads\\\\Evaluating_and_Enhancing_LLM_Generated_Test_Suites\\\\canonical_solutions\"\n",
    "    \n",
    "    # 获取文件列表并确保正确排序\n",
    "    test_files = sorted([f for f in os.listdir(test_dir) if f.startswith(\"HumanEval_\") and f.endswith(\".py\")])\n",
    "    solution_files = sorted([f for f in os.listdir(solution_dir) if f.endswith(\".py\")])\n",
    "    \n",
    "    # 确保文件数量正确且对应\n",
    "    if len(test_files) != 164 or len(solution_files) != 164:\n",
    "        print(f\"警告: 期望164个文件，但找到 {len(test_files)} 个测试文件和 {len(solution_files)} 个解决方案文件\")\n",
    "    \n",
    "    print(f\"找到 {len(test_files)} 个测试文件和 {len(solution_files)} 个解决方案文件\")\n",
    "    print(\"开始运行测试...\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(len(test_files)):\n",
    "        # 根据命名规则匹配文件\n",
    "        test_num = f\"HumanEval_{i}.py\"\n",
    "        solution_num = f\"{i:03d}_*.py\"\n",
    "        \n",
    "        # 查找对应的解决方案文件\n",
    "        solution_pattern = os.path.join(solution_dir, f\"{i:03d}_*.py\")\n",
    "        matching_solutions = glob.glob(solution_pattern)\n",
    "        \n",
    "        if test_num in test_files and matching_solutions:\n",
    "            test_file = test_num\n",
    "            solution_file = os.path.basename(matching_solutions[0])\n",
    "        else:\n",
    "            # 如果按数字匹配失败，按顺序匹配\n",
    "            test_file = test_files[i] if i < len(test_files) else None\n",
    "            solution_file = solution_files[i] if i < len(solution_files) else None\n",
    "        \n",
    "        if not test_file or not solution_file:\n",
    "            print(f\"跳过索引 {i}: 无法找到对应的文件\")\n",
    "            continue\n",
    "            \n",
    "        test_path = os.path.join(test_dir, test_file)\n",
    "        solution_path = os.path.join(solution_dir, solution_file)\n",
    "        \n",
    "        # print(f\"测试 {i+1:3d}/164: {test_file} -> {solution_file}\")\n",
    "        \n",
    "        # 运行测试\n",
    "        passed_tests = run_single_test(solution_path, test_path)\n",
    "        validity_rate = passed_tests / 10.0\n",
    "        \n",
    "        results.append({\n",
    "            'test_id': i,\n",
    "            'test_file': test_file,\n",
    "            'solution_file': solution_file,\n",
    "            'passed_tests': passed_tests,\n",
    "            'total_tests': 10,\n",
    "            'validity_rate': validity_rate\n",
    "        })\n",
    "        \n",
    "        # print(f\"  结果: {passed_tests}/10 通过, Validity Rate: {validity_rate:.2f}\")\n",
    "    \n",
    "    # 创建DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # 计算总体统计\n",
    "    total_passed = df['passed_tests'].sum()\n",
    "    total_tests = len(df) * 10\n",
    "    overall_validity = total_passed / total_tests if total_tests > 0 else 0\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(f\"=== 总体统计 ===\")\n",
    "    print(f\"总测试文件: {len(df)}\")\n",
    "    print(f\"总通过测试: {total_passed}/{total_tests}\")\n",
    "    print(f\"总体Validity Rate: {overall_validity:.4f} ({overall_validity*100:.2f}%)\")\n",
    "    print(f\"平均Validity Rate: {df['validity_rate'].mean():.4f}\")\n",
    "    print(f\"通过率分布:\")\n",
    "    print(df['passed_tests'].value_counts().sort_index())\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 在Jupyter中运行完整测试\n",
    "print(\"开始运行164个测试...\")\n",
    "df_results = calculate_validity_rate_all_tests()\n",
    "\n",
    "# 显示结果\n",
    "print(\"\\n前10行结果:\")\n",
    "# display(df_results.head(10))\n",
    "df_results\n",
    "\n",
    "# # 显示统计摘要\n",
    "# print(\"\\n统计摘要:\")\n",
    "# print(df_results['validity_rate'].describe())\n",
    "\n",
    "# 保存结果\n",
    "# output_file = 'validity_results_complete.csv'\n",
    "# df_results.to_csv(output_file, index=False)\n",
    "# print(f\"\\n完整结果已保存到: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d1462b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Coverage Rate: 63.41%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>entry_point</th>\n",
       "      <th>statements</th>\n",
       "      <th>missing</th>\n",
       "      <th>coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>has_close_elements</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>77.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>separate_paren_groups</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>87.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>truncate_number</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>below_zero</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>75.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>mean_absolute_deviation</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>159</td>\n",
       "      <td>eat</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>75.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>160</td>\n",
       "      <td>do_algebra</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>161</td>\n",
       "      <td>solve</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>93.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>162</td>\n",
       "      <td>string_to_md5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>66.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>163</td>\n",
       "      <td>generate_integers</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>75.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id              entry_point  statements  missing  coverage\n",
       "0      0       has_close_elements           9        2     77.78\n",
       "1      1    separate_paren_groups          16        2     87.50\n",
       "2      2          truncate_number           2        1     50.00\n",
       "3      3               below_zero           8        2     75.00\n",
       "4      4  mean_absolute_deviation           4        2     50.00\n",
       "..   ...                      ...         ...      ...       ...\n",
       "159  159                      eat           4        1     75.00\n",
       "160  160               do_algebra           0        0      0.00\n",
       "161  161                    solve          15        1     93.33\n",
       "162  162            string_to_md5           3        1     66.67\n",
       "163  163        generate_integers           4        1     75.00\n",
       "\n",
       "[164 rows x 5 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import builtins\n",
    "import importlib.util\n",
    "import io\n",
    "import json\n",
    "import pathlib\n",
    "import re\n",
    "import sys\n",
    "import types\n",
    "import unittest\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from coverage import Coverage\n",
    "from coverage.exceptions import CoverageWarning\n",
    "\n",
    "CANON_DIR = pathlib.Path(\"canonical_solutions\").resolve()\n",
    "TESTS_DIR = pathlib.Path(\"generated_tests\").resolve()\n",
    "\n",
    "assert CANON_DIR.is_dir(), \"canonical_solutions/ missing\"\n",
    "assert TESTS_DIR.is_dir(), \"generated_tests/ missing\"\n",
    "\n",
    "def load_solution_as_module(sol_path: pathlib.Path) -> types.ModuleType:\n",
    "    \"\"\"\n",
    "    Read the canonical solution and execute it into a fresh module.\n",
    "    IMPORTANT: compile with filename=str(sol_path) so Coverage attributes lines to this file.\n",
    "    \"\"\"\n",
    "    code = sol_path.read_text(encoding=\"utf-8\")\n",
    "    mod = types.ModuleType(\"candidate\")  # name isn't important for coverage\n",
    "    compiled = compile(code, filename=str(sol_path), mode=\"exec\")\n",
    "    exec(compiled, mod.__dict__)\n",
    "    return mod\n",
    "\n",
    "def load_test_module(test_path: pathlib.Path, entry_point_func) -> types.ModuleType:\n",
    "    \"\"\"\n",
    "    Load the test file as a module, pre-injecting the target function into its globals.\n",
    "    Also put the function into builtins so bare calls resolve anywhere.\n",
    "    \"\"\"\n",
    "    # Safety net: make the function visible as a builtin\n",
    "    setattr(builtins, entry_point_func.__name__, entry_point_func)\n",
    "\n",
    "    spec = importlib.util.spec_from_file_location(\"test_generated\", str(test_path))\n",
    "    if spec is None or spec.loader is None:\n",
    "        raise RuntimeError(f\"Cannot load tests from {test_path}\")\n",
    "    test_mod = importlib.util.module_from_spec(spec)\n",
    "\n",
    "    # Inject before executing the test file\n",
    "    test_mod.__dict__[entry_point_func.__name__] = entry_point_func\n",
    "\n",
    "    # Execute test module\n",
    "    sys.modules[\"test_generated\"] = test_mod\n",
    "    spec.loader.exec_module(test_mod)\n",
    "    return test_mod\n",
    "\n",
    "def run_tests_get_suite(test_mod: types.ModuleType) -> unittest.TestSuite:\n",
    "    \"\"\"Create a suite from the already-loaded test module.\"\"\"\n",
    "    loader = unittest.defaultTestLoader\n",
    "    return loader.loadTestsFromModule(test_mod)\n",
    "\n",
    "def coverage_percent_for_file(cov: Coverage, filename: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute percent covered for a specific filename using Coverage API.\n",
    "    Returns 0.0 if file wasn't measured.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = cov.get_data()\n",
    "        if not data or filename not in data.measured_files():\n",
    "            return 0.0, 0, 0\n",
    "        # analysis2 returns (filename, statements, excluded, missing, partial)\n",
    "        _, statements, _, missing, _ = cov.analysis2(filename)\n",
    "        if not statements:\n",
    "            return 0.0\n",
    "        covered = len(statements) - len(missing)\n",
    "        return round(100.0 * covered / len(statements), 2), len(statements), len(missing)\n",
    "    except CoverageWarning:\n",
    "        return 0.0, 0, 0\n",
    "\n",
    "rows = []\n",
    "\n",
    "# Discover (idx, entry_point, solution_path)\n",
    "canon = []\n",
    "for f in sorted(CANON_DIR.glob(\"*.py\")):\n",
    "    m = re.match(r\"^(\\d{3})_(.+)\\.py$\", f.name)\n",
    "    if m:\n",
    "        canon.append((int(m.group(1)), m.group(2), f))\n",
    "\n",
    "for idx, entry_point, sol_path in canon:\n",
    "    test_path = TESTS_DIR / f\"HumanEval_{idx}.py\"\n",
    "    if not test_path.exists():\n",
    "        print(f\"[SKIP] {idx:03d} no test file {test_path.name}\")\n",
    "        continue\n",
    "\n",
    "    # 1) Load solution module and fetch the function\n",
    "    sol_mod = load_solution_as_module(sol_path)\n",
    "    if not hasattr(sol_mod, entry_point):\n",
    "        print(f\"[WARN] {idx:03d} entry point '{entry_point}' not found in {sol_path.name}\")\n",
    "        func = None\n",
    "    else:\n",
    "        func = getattr(sol_mod, entry_point)\n",
    "\n",
    "    # 2) Load tests with function injected\n",
    "    if func is None:\n",
    "        coverage_pct = 0.0\n",
    "    else:\n",
    "        test_mod = load_test_module(test_path, func)\n",
    "        suite = run_tests_get_suite(test_mod)\n",
    "\n",
    "        # 3) Run tests under Coverage API, measuring ONLY the target file via include=[...]\n",
    "        cov = Coverage(include=[str(sol_path)])\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=CoverageWarning)\n",
    "\n",
    "        cov.erase()\n",
    "        cov.start()\n",
    "        \n",
    "        # run tests\n",
    "        result = unittest.TextTestRunner(stream=io.StringIO(), verbosity=0).run(suite)\n",
    "        cov.stop()\n",
    "        cov.save()\n",
    "        # cov.report()  # initialize coverage\n",
    "\n",
    "        # 4) Compute percent for this exact file\n",
    "        coverage_pct, statements, missing = coverage_percent_for_file(cov, str(sol_path))\n",
    "\n",
    "    # print(f\"[{idx:03d}] cov={coverage_pct:.1f}%\")\n",
    "    rows.append({\n",
    "        \"id\": idx,\n",
    "        \"entry_point\": entry_point,\n",
    "        \"statements\": statements,\n",
    "        \"missing\": missing,\n",
    "        \"coverage\": coverage_pct,\n",
    "    })\n",
    "    # if(idx == 2):\n",
    "\n",
    "    #     break\n",
    "\n",
    "df_cov = pd.DataFrame(rows).sort_values(\"id\").reset_index(drop=True)\n",
    "print(f\"Average Coverage Rate: {df_cov['coverage'].mean():.2f}%\")\n",
    "df_cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "369b0cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "运行调试版本...\n",
      "Test result: False\n",
      "Name                                            Stmts   Miss  Cover\n",
      "-------------------------------------------------------------------\n",
      "canonical_solutions\\000_has_close_elements.py       9      3    67%\n",
      "-------------------------------------------------------------------\n",
      "TOTAL                                               9      3    67%\n"
     ]
    }
   ],
   "source": [
    "import coverage\n",
    "import os\n",
    "\n",
    "# 最简单的测试版本\n",
    "def debug_coverage():\n",
    "    cov = coverage.Coverage()\n",
    "    cov.start()\n",
    "    \n",
    "    # 直接执行一个简单的测试\n",
    "    test_code = '''\n",
    "mod = importlib.import_module(\n",
    "    \"canonical_solutions.000_has_close_elements\"\n",
    ")\n",
    "has_close_elements = mod.has_close_elements\n",
    "\n",
    "# 简单测试\n",
    "result = has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
    "print(f\"Test result: {result}\")\n",
    "'''\n",
    "    \n",
    "    exec(test_code)\n",
    "    \n",
    "    cov.stop()\n",
    "    cov.save()\n",
    "    cov.report()\n",
    "    # cov.html_report()\n",
    "\n",
    "# 先运行这个调试版本\n",
    "print(\"运行调试版本...\")\n",
    "debug_coverage()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b8c13de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pathlib\n",
    "from datasets import load_dataset\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f189148",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gemini-2.5-flash\"\n",
    "# OUT_DIR = pathlib.Path(\"generated_tests\")\n",
    "# OUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae0b1e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_docstring(prompt_text: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract all triple-quoted docstrings from the input text.\n",
    "    Returns a list of docstring contents as strings.\n",
    "    \"\"\"\n",
    "    matches = re.findall(r'(\"\"\"|\\'\\'\\')(.*?)(\\1)', prompt_text, flags=re.DOTALL)\n",
    "    # Join all docstrings into a single string (if needed)\n",
    "    doc_text = \"\\n\\n\".join([m[1].strip() for m in matches])\n",
    "\n",
    "    return doc_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6184636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HumanEval with 164 problems.\n"
     ]
    }
   ],
   "source": [
    "api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"Please set GOOGLE_API_KEY in your environment.\")\n",
    "genai.configure(api_key=api_key)\n",
    "model = genai.GenerativeModel(MODEL)\n",
    "\n",
    "ds = load_dataset(\"openai/openai_humaneval\")[\"test\"]  # 164 items\n",
    "print(f\"Loaded HumanEval with {len(ds)} problems.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d6520e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\n",
      "Evaluates polynomial with coefficients xs at point x.\n",
      "    return xs[0] + xs[1] * x + xs[1] * x^2 + .... xs[n] * x^n\n",
      "\n",
      "xs are coefficients of a polynomial.\n",
      "    find_zero find x such that poly(x) = 0.\n",
      "    find_zero returns only only zero point, even if there are many.\n",
      "    Moreover, find_zero only takes list xs having even number of coefficients\n",
      "    and largest non zero coefficient as it guarantees\n",
      "    a solution.\n",
      "    >>> round(find_zero([1, 2]), 2) # f(x) = 1 + 2x\n",
      "    -0.5\n",
      "    >>> round(find_zero([-6, 11, -6, 1]), 2) # (x - 1) * (x - 2) * (x - 3) = -6 + 11x - 6x^2 + x^3\n",
      "    1.0\n",
      "\"\"\"\n",
      "Please generate 10 test cases in Python's standard unittest format for this problem. Please ONLY generate test cases, assume the function exist.\n",
      "Generation Complete\n"
     ]
    }
   ],
   "source": [
    "for idx, item in enumerate(ds):\n",
    "    doc = extract_docstring(item[\"prompt\"])\n",
    "    prompt = f'\"\"\"\\n{doc}\\n\"\"\"\\nPlease generate 10 test cases in Python\\'s standard unittest format for this problem. Please ONLY generate test cases, assume the function exist.'\n",
    "    if(idx == 32):\n",
    "        print(prompt)\n",
    "    # uncomment to run generation\n",
    "    # try:\n",
    "    #     resp = model.generate_content(prompt)\n",
    "    #     text = (resp.text or \"\").strip()\n",
    "    #     # If fences sneak in, strip them.\n",
    "    #     text = re.sub(r\"^```(?:python)?\\s*\", \"\", text)\n",
    "    #     text = re.sub(r\"\\s*```$\", \"\", text)\n",
    "\n",
    "    #     out_path = OUT_DIR / f\"HumanEval_{idx}.py\"\n",
    "    #     out_path.write_text(text, encoding=\"utf-8\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"[WARN] Problem {idx} failed: {e}\")\n",
    "        \n",
    "print(\"Generation Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7401322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 164 canonical solutions...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pathlib\n",
    "import textwrap\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# === CONFIG ===\n",
    "OUT_DIR = pathlib.Path(\"canonical_solutions\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "INCLUDE_IMPORTS = True  # set False to drop import lines and keep only `def ...:` + body\n",
    "\n",
    "def extract_signature_from_prompt(prompt: str, include_imports: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    From HumanEval `prompt`, remove the triple-quoted docstring and 'pass',\n",
    "    then return the remaining code (imports + def line).\n",
    "    \"\"\"\n",
    "    # remove first triple-quoted docstring block (\"\"\"...\"\"\" or '''...''')\n",
    "    code_wo_doc = re.sub(r'(\"\"\"|\\'\\'\\')(.*?)(\\1)', '', prompt, flags=re.DOTALL)\n",
    "    # remove any bare 'pass' lines\n",
    "    code_wo_doc = re.sub(r'^[ \\t]*pass[ \\t]*\\r?\\n?', '', code_wo_doc, flags=re.MULTILINE)\n",
    "    # normalize whitespace\n",
    "    lines = [ln.rstrip() for ln in code_wo_doc.strip().splitlines() if ln.strip()]\n",
    "\n",
    "    if not lines:\n",
    "        raise ValueError(\"No signature content found in prompt.\")\n",
    "\n",
    "    # Find the first def line\n",
    "    def_idx = next((i for i, ln in enumerate(lines) if ln.lstrip().startswith(\"def \")), None)\n",
    "    if def_idx is None:\n",
    "        # fallback: sometimes there's a blank before def; just join everything\n",
    "        joined = \"\\n\".join(lines) + (\"\\n\" if not lines[-1].endswith(\"\\n\") else \"\")\n",
    "        return joined\n",
    "\n",
    "    if include_imports:\n",
    "        kept = lines[:def_idx+1]  # imports (if any) + the def line\n",
    "    else:\n",
    "        kept = [lines[def_idx]]   # only the def line\n",
    "\n",
    "    sig = \"\\n\".join(kept)\n",
    "    if not sig.endswith(\"\\n\"):\n",
    "        sig += \"\\n\"\n",
    "    return sig\n",
    "\n",
    "def assemble_module(signature_code: str, body_code: str) -> str:\n",
    "    \"\"\"\n",
    "    Indent the canonical_solution body under the def line.\n",
    "    Handles cases where body is already (or not) indented.\n",
    "    \"\"\"\n",
    "    body = textwrap.dedent(body_code.rstrip(\"\\n\")) + \"\\n\"\n",
    "    body_indented = textwrap.indent(body, \"    \")\n",
    "    return signature_code + body_indented\n",
    "\n",
    "def get_split(ds_any) -> Dataset:\n",
    "    \"\"\"\n",
    "    Accept either:\n",
    "      - a DatasetDict with key 'test'\n",
    "      - a Dataset that is already the split\n",
    "    \"\"\"\n",
    "    if isinstance(ds_any, DatasetDict):\n",
    "        return ds_any[\"test\"]\n",
    "    if isinstance(ds_any, Dataset):\n",
    "        return ds_any\n",
    "    raise TypeError(\"Provide a Hugging Face Dataset or DatasetDict (with 'test').\")\n",
    "\n",
    "try:\n",
    "    split = get_split(ds)       # if your variable is named ds\n",
    "except NameError:\n",
    "    ds = load_dataset(\"openai/openai_humaneval\")[\"test\"]  # 164 items\n",
    "    split = get_split(ds) \n",
    "\n",
    "print(f\"Writing {len(split)} canonical solutions...\")\n",
    "\n",
    "for i, item in enumerate(split):\n",
    "    task_id = item.get(\"task_id\", f\"HumanEval/{i}\")\n",
    "    entry_point = item.get(\"entry_point\", f\"task_{i}\")\n",
    "    prompt = item[\"prompt\"]\n",
    "    body = item[\"canonical_solution\"]\n",
    "\n",
    "    # 1) signature\n",
    "    signature = extract_signature_from_prompt(prompt, include_imports=INCLUDE_IMPORTS)\n",
    "\n",
    "    # 2) final module\n",
    "    final_code = assemble_module(signature, body)\n",
    "\n",
    "    # 3) filename like 000_entrypoint.py\n",
    "    idx_str = str(i).zfill(3)\n",
    "    out_path = OUT_DIR / f\"{idx_str}_{entry_point}.py\"\n",
    "    out_path.write_text(final_code, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183de104",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "02fa38d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 164 test files 164 solution files\n",
      "Test Start...\n",
      "\n",
      "Average Validity Rate: 0.8085\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>test_file</th>\n",
       "      <th>solution_file</th>\n",
       "      <th>passed_tests</th>\n",
       "      <th>total_tests</th>\n",
       "      <th>validity_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>HumanEval_0.py</td>\n",
       "      <td>000_has_close_elements.py</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>HumanEval_1.py</td>\n",
       "      <td>001_separate_paren_groups.py</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>HumanEval_2.py</td>\n",
       "      <td>002_truncate_number.py</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>HumanEval_3.py</td>\n",
       "      <td>003_below_zero.py</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>HumanEval_4.py</td>\n",
       "      <td>004_mean_absolute_deviation.py</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>159</td>\n",
       "      <td>HumanEval_159.py</td>\n",
       "      <td>159_eat.py</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>160</td>\n",
       "      <td>HumanEval_160.py</td>\n",
       "      <td>160_do_algebra.py</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>161</td>\n",
       "      <td>HumanEval_161.py</td>\n",
       "      <td>161_solve.py</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>162</td>\n",
       "      <td>HumanEval_162.py</td>\n",
       "      <td>162_string_to_md5.py</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>163</td>\n",
       "      <td>HumanEval_163.py</td>\n",
       "      <td>163_generate_integers.py</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     test_id         test_file                   solution_file  passed_tests  \\\n",
       "0          0    HumanEval_0.py       000_has_close_elements.py            10   \n",
       "1          1    HumanEval_1.py    001_separate_paren_groups.py             9   \n",
       "2          2    HumanEval_2.py          002_truncate_number.py            10   \n",
       "3          3    HumanEval_3.py               003_below_zero.py            10   \n",
       "4          4    HumanEval_4.py  004_mean_absolute_deviation.py             9   \n",
       "..       ...               ...                             ...           ...   \n",
       "159      159  HumanEval_159.py                      159_eat.py            10   \n",
       "160      160  HumanEval_160.py               160_do_algebra.py             0   \n",
       "161      161  HumanEval_161.py                    161_solve.py            10   \n",
       "162      162  HumanEval_162.py            162_string_to_md5.py             0   \n",
       "163      163  HumanEval_163.py        163_generate_integers.py             7   \n",
       "\n",
       "     total_tests  validity_rate  \n",
       "0             10            1.0  \n",
       "1             10            0.9  \n",
       "2             10            1.0  \n",
       "3             10            1.0  \n",
       "4             10            0.9  \n",
       "..           ...            ...  \n",
       "159           10            1.0  \n",
       "160           10            0.0  \n",
       "161           10            1.0  \n",
       "162           10            0.0  \n",
       "163           10            0.7  \n",
       "\n",
       "[164 rows x 6 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import tempfile\n",
    "import re\n",
    "import glob\n",
    "\n",
    "def extract_test_class_name(test_code):\n",
    "    \"\"\"extract class information from the test file\"\"\"\n",
    "    class_match = re.search(r'class\\s+(\\w+)\\(unittest\\.TestCase\\)', test_code)\n",
    "    if class_match:\n",
    "        return class_match.group(1)\n",
    "    else:\n",
    "        # if can not extract, try other way\n",
    "        class_match = re.search(r'class\\s+(\\w+)\\s*\\(.*TestCase.*\\)', test_code)\n",
    "        if class_match:\n",
    "            return class_match.group(1)\n",
    "    return None\n",
    "\n",
    "def run_single_test(solution_file, test_file):\n",
    "    \"\"\"run single test instance\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # read solution file\n",
    "        with open(solution_file, 'r', encoding='utf-8') as f:\n",
    "            solution_code = f.read()\n",
    "        \n",
    "        # read test file\n",
    "        with open(test_file, 'r', encoding='utf-8') as f:\n",
    "            test_code = f.read()\n",
    "        \n",
    "        # extract test class name\n",
    "        test_class_name = extract_test_class_name(test_code)\n",
    "        if not test_class_name:\n",
    "            print(f\"  warning: can not extract class name from {os.path.basename(test_file)}\")\n",
    "            return 0\n",
    "        \n",
    "        # creat temp fileï¼Œmerge solution and test\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False, encoding='utf-8') as temp_file:\n",
    "            # write solution code\n",
    "            temp_file.write(solution_code + \"\\n\\n\")\n",
    "            \n",
    "            # write test code\n",
    "            temp_file.write(test_code + \"\\n\\n\")\n",
    "            \n",
    "            # add code to excute test\n",
    "            temp_file.write(f\"\"\"\n",
    "import sys\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import unittest\n",
    "    try:\n",
    "        suite = unittest.TestLoader().loadTestsFromTestCase({test_class_name})\n",
    "        runner = unittest.TextTestRunner(verbosity=0, stream=sys.stdout)\n",
    "        result = runner.run(suite)\n",
    "        \n",
    "        # output statistic\n",
    "        print(f\"TESTS_RUN:{{result.testsRun}}\")\n",
    "        print(f\"FAILURES:{{len(result.failures)}}\")\n",
    "        print(f\"ERRORS:{{len(result.errors)}}\")\n",
    "        print(f\"PASSED:{{result.testsRun - len(result.failures) - len(result.errors)}}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR running tests: {{e}}\")\n",
    "        print(\"TESTS_RUN:0\")\n",
    "        print(\"FAILURES:0\")\n",
    "        print(\"ERRORS:1\")\n",
    "        print(\"PASSED:0\")\n",
    "\"\"\")\n",
    "            temp_file_path = temp_file.name\n",
    "        \n",
    "        # run test\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, temp_file_path],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=30  # 30s timeout\n",
    "        )\n",
    "        \n",
    "        # parse output\n",
    "        output = result.stdout + result.stderr\n",
    "        \n",
    "        # extract pass number from output\n",
    "        passed_match = re.search(r'PASSED:(\\d+)', output)\n",
    "        tests_run_match = re.search(r'TESTS_RUN:(\\d+)', output)\n",
    "        \n",
    "        if passed_match and tests_run_match:\n",
    "            passed_tests = int(passed_match.group(1))\n",
    "            tests_run = int(tests_run_match.group(1))\n",
    "            \n",
    "            return passed_tests\n",
    "        else:\n",
    "            # if can't parse the output\n",
    "            if \"OK\" in output and (\"FAILED\" not in output or \"FAILURES: 0\" in output):\n",
    "                return 10\n",
    "            else:\n",
    "                # count fail and error\n",
    "                failures = len(re.findall(r'FAIL:', output)) + len(re.findall(r'ERROR:', output))\n",
    "                errors = len(re.findall(r'Traceback', output))\n",
    "                return max(0, 10 - failures - errors)\n",
    "                \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"  Timeout: {os.path.basename(test_file)}\")\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(f\"  Error in running test {os.path.basename(test_file)}: {e}\")\n",
    "        return 0\n",
    "    finally:\n",
    "        # remove temp file\n",
    "        try:\n",
    "            if 'temp_file_path' in locals():\n",
    "                os.unlink(temp_file_path)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "def calculate_validity_rate_all_tests():\n",
    "    \"\"\"Calculate Validity Rate for all files\"\"\"\n",
    "    \n",
    "    test_dir = \"C:\\\\Users\\\\zhang\\\\Downloads\\\\Evaluating_and_Enhancing_LLM_Generated_Test_Suites\\\\generated_tests\"\n",
    "    solution_dir = \"C:\\\\Users\\\\zhang\\\\Downloads\\\\Evaluating_and_Enhancing_LLM_Generated_Test_Suites\\\\canonical_solutions\"\n",
    "    \n",
    "    # get file list with proper ordering\n",
    "    test_files = sorted([f for f in os.listdir(test_dir) if f.startswith(\"HumanEval_\") and f.endswith(\".py\")])\n",
    "    solution_files = sorted([f for f in os.listdir(solution_dir) if f.endswith(\".py\")])\n",
    "    \n",
    "    # make sure the file number is correct\n",
    "    if len(test_files) != 164 or len(solution_files) != 164:\n",
    "        print(f\"warning: expect 164 files, but find {len(test_files)} test files {len(solution_files)} solution files\")\n",
    "    \n",
    "    print(f\"Found {len(test_files)} test files {len(solution_files)} solution files\")\n",
    "    print(\"Test Start...\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(len(test_files)):\n",
    "        # mapping files based on name\n",
    "        test_num = f\"HumanEval_{i}.py\"\n",
    "        solution_num = f\"{i:03d}_*.py\"\n",
    "        \n",
    "        # find corresponding solution file\n",
    "        solution_pattern = os.path.join(solution_dir, f\"{i:03d}_*.py\")\n",
    "        matching_solutions = glob.glob(solution_pattern)\n",
    "        \n",
    "        if test_num in test_files and matching_solutions:\n",
    "            test_file = test_num\n",
    "            solution_file = os.path.basename(matching_solutions[0])\n",
    "        else:\n",
    "            # if failed mapping by numberï¼Œmap by order\n",
    "            test_file = test_files[i] if i < len(test_files) else None\n",
    "            solution_file = solution_files[i] if i < len(solution_files) else None\n",
    "        \n",
    "        if not test_file or not solution_file:\n",
    "            print(f\"Skip index {i}: can not file file\")\n",
    "            continue\n",
    "            \n",
    "        test_path = os.path.join(test_dir, test_file)\n",
    "        solution_path = os.path.join(solution_dir, solution_file)\n",
    "        \n",
    "        # print(f\"Testing: {i+1:3d}/164: {test_file} -> {solution_file}\")\n",
    "        \n",
    "        # Run test\n",
    "        passed_tests = run_single_test(solution_path, test_path)\n",
    "        validity_rate = passed_tests / 10.0\n",
    "        \n",
    "        results.append({\n",
    "            'test_id': i,\n",
    "            'test_file': test_file,\n",
    "            'solution_file': solution_file,\n",
    "            'passed_tests': passed_tests,\n",
    "            'total_tests': 10,\n",
    "            'validity_rate': validity_rate\n",
    "        })\n",
    "        \n",
    "        # print(f\"  Result: {passed_tests}/10 passed, Validity Rate: {validity_rate:.2f}\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_passed = df['passed_tests'].sum()\n",
    "    total_tests = len(df) * 10\n",
    "    overall_validity = total_passed / total_tests if total_tests > 0 else 0\n",
    "    \n",
    "    # print(f\"\\n\" + \"=\"*50)\n",
    "    # print(f\"=== Overall Summary ===\")\n",
    "    # print(f\"Number of test files: {len(df)}\")\n",
    "    # print(f\"Passed tests: {total_passed}/{total_tests}\")\n",
    "    # print(f\"Overall Validity Rate: {overall_validity:.4f} ({overall_validity*100:.2f}%)\")\n",
    "    print(f\"Average Validity Rate: {df['validity_rate'].mean():.4f}\")\n",
    "    # print(f\"Pass distribution:\")\n",
    "    # print(df['passed_tests'].value_counts().sort_index())\n",
    "    # print(\"=\"*50)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run full tests\n",
    "df_results = calculate_validity_rate_all_tests()\n",
    "\n",
    "# Display result\n",
    "df_results\n",
    "\n",
    "# # show description\n",
    "# print(\"\\nDescription:\")\n",
    "# print(df_results['validity_rate'].describe())\n",
    "\n",
    "# save the result to csv\n",
    "# output_file = 'validity_results_complete.csv'\n",
    "# df_results.to_csv(output_file, index=False)\n",
    "# print(f\"\\nSaved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1462b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Coverage Rate: 63.41%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>entry_point</th>\n",
       "      <th>statements</th>\n",
       "      <th>missing</th>\n",
       "      <th>coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>has_close_elements</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>77.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>separate_paren_groups</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>87.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>truncate_number</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>below_zero</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>75.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>mean_absolute_deviation</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>159</td>\n",
       "      <td>eat</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>75.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>160</td>\n",
       "      <td>do_algebra</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>161</td>\n",
       "      <td>solve</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>93.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>162</td>\n",
       "      <td>string_to_md5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>66.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>163</td>\n",
       "      <td>generate_integers</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>75.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id              entry_point  statements  missing  coverage\n",
       "0      0       has_close_elements           9        2     77.78\n",
       "1      1    separate_paren_groups          16        2     87.50\n",
       "2      2          truncate_number           2        1     50.00\n",
       "3      3               below_zero           8        2     75.00\n",
       "4      4  mean_absolute_deviation           4        2     50.00\n",
       "..   ...                      ...         ...      ...       ...\n",
       "159  159                      eat           4        1     75.00\n",
       "160  160               do_algebra           0        0      0.00\n",
       "161  161                    solve          15        1     93.33\n",
       "162  162            string_to_md5           3        1     66.67\n",
       "163  163        generate_integers           4        1     75.00\n",
       "\n",
       "[164 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import builtins\n",
    "import importlib.util\n",
    "import io\n",
    "import json\n",
    "import pathlib\n",
    "import re\n",
    "import sys\n",
    "import types\n",
    "import unittest\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from coverage import Coverage\n",
    "from coverage.exceptions import CoverageWarning\n",
    "warnings.filterwarnings(\"ignore\", category=CoverageWarning)\n",
    "\n",
    "CANON_DIR = pathlib.Path(\"canonical_solutions\").resolve()\n",
    "TESTS_DIR = pathlib.Path(\"old_generated_tests\").resolve()\n",
    "\n",
    "assert CANON_DIR.is_dir(), \"canonical_solutions/ missing\"\n",
    "assert TESTS_DIR.is_dir(), \"generated_tests/ missing\"\n",
    "\n",
    "def load_solution_as_module(sol_path: pathlib.Path) -> types.ModuleType:\n",
    "    \"\"\"\n",
    "    Read the canonical solution and execute it into a fresh module.\n",
    "    IMPORTANT: compile with filename=str(sol_path) so Coverage attributes lines to this file.\n",
    "    \"\"\"\n",
    "    code = sol_path.read_text(encoding=\"utf-8\")\n",
    "    mod = types.ModuleType(\"candidate\")  # name isn't important for coverage\n",
    "    compiled = compile(code, filename=str(sol_path), mode=\"exec\")\n",
    "    exec(compiled, mod.__dict__)\n",
    "    return mod\n",
    "\n",
    "def load_test_module(test_path: pathlib.Path, entry_point_func) -> types.ModuleType:\n",
    "    \"\"\"\n",
    "    Load the test file as a module, pre-injecting the target function into its globals.\n",
    "    Also put the function into builtins so bare calls resolve anywhere.\n",
    "    \"\"\"\n",
    "    # Safety net: make the function visible as a builtin\n",
    "    setattr(builtins, entry_point_func.__name__, entry_point_func)\n",
    "\n",
    "    spec = importlib.util.spec_from_file_location(\"test_generated\", str(test_path))\n",
    "    if spec is None or spec.loader is None:\n",
    "        raise RuntimeError(f\"Cannot load tests from {test_path}\")\n",
    "    test_mod = importlib.util.module_from_spec(spec)\n",
    "\n",
    "    # Inject before executing the test file\n",
    "    test_mod.__dict__[entry_point_func.__name__] = entry_point_func\n",
    "\n",
    "    # Execute test module\n",
    "    sys.modules[\"test_generated\"] = test_mod\n",
    "    spec.loader.exec_module(test_mod)\n",
    "    return test_mod\n",
    "\n",
    "def run_tests_get_suite(test_mod: types.ModuleType) -> unittest.TestSuite:\n",
    "    \"\"\"Create a suite from the already-loaded test module.\"\"\"\n",
    "    loader = unittest.defaultTestLoader\n",
    "    return loader.loadTestsFromModule(test_mod)\n",
    "\n",
    "def coverage_percent_for_file(cov: Coverage, filename: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute percent covered for a specific filename using Coverage API.\n",
    "    Returns 0.0 if file wasn't measured.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = cov.get_data()\n",
    "        if not data or filename not in data.measured_files():\n",
    "            return 0.0, 0, 0\n",
    "        # analysis2 returns (filename, statements, excluded, missing, partial)\n",
    "        _, statements, _, missing, _ = cov.analysis2(filename)\n",
    "        if not statements:\n",
    "            return 0.0\n",
    "        covered = len(statements) - len(missing)\n",
    "        return round(100.0 * covered / len(statements), 2), len(statements), len(missing)\n",
    "    except CoverageWarning:\n",
    "        return 0.0, 0, 0\n",
    "\n",
    "rows = []\n",
    "\n",
    "# Discover (idx, entry_point, solution_path)\n",
    "canon = []\n",
    "for f in sorted(CANON_DIR.glob(\"*.py\")):\n",
    "    m = re.match(r\"^(\\d{3})_(.+)\\.py$\", f.name)\n",
    "    if m:\n",
    "        canon.append((int(m.group(1)), m.group(2), f))\n",
    "\n",
    "for idx, entry_point, sol_path in canon:\n",
    "    test_path = TESTS_DIR / f\"HumanEval_{idx}.py\"\n",
    "    if not test_path.exists():\n",
    "        print(f\"[SKIP] {idx:03d} no test file {test_path.name}\")\n",
    "        continue\n",
    "\n",
    "    # 1) Load solution module and fetch the function\n",
    "    sol_mod = load_solution_as_module(sol_path)\n",
    "    if not hasattr(sol_mod, entry_point):\n",
    "        print(f\"[WARN] {idx:03d} entry point '{entry_point}' not found in {sol_path.name}\")\n",
    "        func = None\n",
    "    else:\n",
    "        func = getattr(sol_mod, entry_point)\n",
    "\n",
    "    # 2) Load tests with function injected\n",
    "    if func is None:\n",
    "        coverage_pct = 0.0\n",
    "    else:\n",
    "        test_mod = load_test_module(test_path, func)\n",
    "        suite = run_tests_get_suite(test_mod)\n",
    "\n",
    "        # 3) Run tests under Coverage API, measuring ONLY the target file via include=[...]\n",
    "\n",
    "        cov = Coverage(include=[str(sol_path)])\n",
    "\n",
    "        cov.erase()\n",
    "        cov.start()\n",
    "        \n",
    "        # run tests\n",
    "        result = unittest.TextTestRunner(stream=io.StringIO(), verbosity=0).run(suite)\n",
    "        cov.stop()\n",
    "        cov.save()\n",
    "        # cov.report()  # initialize coverage\n",
    "\n",
    "        # 4) Compute percent for this exact file\n",
    "        coverage_pct, statements, missing = coverage_percent_for_file(cov, str(sol_path))\n",
    "\n",
    "    # print(f\"[{idx:03d}] cov={coverage_pct:.1f}%\")\n",
    "    rows.append({\n",
    "        \"id\": idx,\n",
    "        \"entry_point\": entry_point,\n",
    "        \"statements\": statements,\n",
    "        \"missing\": missing,\n",
    "        \"coverage\": coverage_pct,\n",
    "    })\n",
    "    # if(idx == 2):\n",
    "\n",
    "    #     break\n",
    "\n",
    "df_cov = pd.DataFrame(rows).sort_values(\"id\").reset_index(drop=True)\n",
    "print(f\"Average Coverage Rate: {df_cov['coverage'].mean():.2f}%\")\n",
    "df_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af82003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cov.to_csv(\"C:\\\\Users\\\\zhang\\\\Downloads\\\\coverage.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2444e2f",
   "metadata": {},
   "source": [
    "### ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a8864473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>entry_point</th>\n",
       "      <th>total_mutants</th>\n",
       "      <th>killed_mutants</th>\n",
       "      <th>mutation_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>has_close_elements</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>separate_paren_groups</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>truncate_number</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>below_zero</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>mean_absolute_deviation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>159</td>\n",
       "      <td>eat</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>160</td>\n",
       "      <td>do_algebra</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>161</td>\n",
       "      <td>solve</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>89.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>162</td>\n",
       "      <td>string_to_md5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>163</td>\n",
       "      <td>generate_integers</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id              entry_point  total_mutants  killed_mutants  \\\n",
       "0      0       has_close_elements              6               6   \n",
       "1      1    separate_paren_groups              0               0   \n",
       "2      2          truncate_number              2               2   \n",
       "3      3               below_zero              8               8   \n",
       "4      4  mean_absolute_deviation              0               0   \n",
       "..   ...                      ...            ...             ...   \n",
       "159  159                      eat              5               4   \n",
       "160  160               do_algebra              0               0   \n",
       "161  161                    solve             19              17   \n",
       "162  162            string_to_md5              0               0   \n",
       "163  163        generate_integers              0               0   \n",
       "\n",
       "     mutation_score  \n",
       "0        100.000000  \n",
       "1          0.000000  \n",
       "2        100.000000  \n",
       "3        100.000000  \n",
       "4          0.000000  \n",
       "..              ...  \n",
       "159       80.000000  \n",
       "160        0.000000  \n",
       "161       89.473684  \n",
       "162        0.000000  \n",
       "163        0.000000  \n",
       "\n",
       "[164 rows x 5 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess, pathlib, re, pandas as pd\n",
    "\n",
    "# Adjust paths\n",
    "CANON_DIR = pathlib.Path(\"canonical_solutions\").resolve()\n",
    "TESTS_DIR = pathlib.Path(\"generated_tests\").resolve()\n",
    "RUNNER = \"/mnt/c/Users/zhang/Downloads/Evaluating_and_Enhancing_LLM_Generated_Test_Suites/_test_runner.py\"\n",
    "WSL_PY = \"/home/zhang/.venvs/humaneval/bin/python\"\n",
    "\n",
    "\n",
    "def to_wsl_path(p: pathlib.Path) -> str:\n",
    "    s = str(p)\n",
    "    if s.startswith(\"/\"):  # already WSL style\n",
    "        return s\n",
    "    drive = p.drive.rstrip(\":\").lower()\n",
    "    tail = s.replace(p.drive, \"\").replace(\"\\\\\", \"/\")\n",
    "    return f\"/mnt/{drive}{tail}\"\n",
    "\n",
    "def wsl_run(cmd: str):\n",
    "    \"\"\"Run a shell command in WSL and return result object with stdout/stderr.\"\"\"\n",
    "    result =subprocess.run(\n",
    "        [\"wsl\", \"-d\", \"Ubuntu\", \"bash\", \"-lc\", cmd],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "    # print(result.stdout if result.stdout.strip() else \"<empty>\")\n",
    "    return result\n",
    "    \n",
    "\n",
    "rows = []\n",
    "\n",
    "for sol in sorted(CANON_DIR.glob(\"*.py\")):\n",
    "    \n",
    "    m = re.match(r\"^(\\d{3})_(.+)\\.py$\", sol.name)\n",
    "    if not m:\n",
    "        continue\n",
    "    idx, entry_point = int(m.group(1)), m.group(2)\n",
    "    test = TESTS_DIR / f\"HumanEval_{idx}.py\"\n",
    "    if not test.exists():\n",
    "        print(f\"[SKIP] {idx:03d} missing {test.name}\")\n",
    "        continue\n",
    "\n",
    "    CANDIDATE_FILE = to_wsl_path(sol)\n",
    "    TEST_FILE = to_wsl_path(test)\n",
    "    \n",
    "    # 1) clear cache\n",
    "    wsl_run(\"rm -rf .mutmut-cache\")\n",
    "\n",
    "    # 2) mutmut run\n",
    "    cmd = (\n",
    "        f'CANDIDATE_FILE={CANDIDATE_FILE} '\n",
    "        f'TEST_FILE={TEST_FILE} '\n",
    "        f'ENTRY_POINT={entry_point} '\n",
    "        f'PYTHONPATH={to_wsl_path(TESTS_DIR)} '\n",
    "        f'{WSL_PY} -m mutmut run --paths-to-mutate \"{CANDIDATE_FILE}\" '\n",
    "        f'--runner \"{WSL_PY} {RUNNER}\" --tests-dir .'\n",
    "    )\n",
    "    result = wsl_run(cmd)\n",
    "\n",
    "    # extract last line of stdout\n",
    "    lines = [ln for ln in result.stdout.splitlines() if ln.strip()]\n",
    "    last_line = lines[-1] if lines else \"\"\n",
    "    m_total = re.search(r\"(\\d+)/(\\d+)\", last_line)\n",
    "    m_killed = re.search(r\"ðŸŽ‰\\s*(\\d+)\", last_line)\n",
    "    if m_total and m_killed:\n",
    "        total = int(m_total.group(2))   # second number in x/y\n",
    "        killed = int(m_killed.group(1))\n",
    "        mutation_score = 100.0 * killed / total if total else 0.0\n",
    "    else:\n",
    "        total, killed, mutation_score = 0, 0, 0.0\n",
    "\n",
    "    # print(f\"[{idx:03d}] {entry_point}: total={total}, killed={killed}, score={mutation_score:.1f}\")\n",
    "    rows.append({\n",
    "        \"id\": idx,\n",
    "        \"entry_point\": entry_point,\n",
    "        \"total_mutants\": total,\n",
    "        \"killed_mutants\": killed,\n",
    "        \"mutation_score\": mutation_score,\n",
    "    })\n",
    "    wsl_run(\"rm -rf .mutmut-cache\")\n",
    "    # if (idx == 1):\n",
    "    #     print(CANDIDATE_FILE, TEST_FILE)\n",
    "    #     print(entry_point)\n",
    "    #     print(to_wsl_path(TESTS_DIR))\n",
    "    #     break\n",
    "\n",
    "df_mut = pd.DataFrame(rows).sort_values(\"id\").reset_index(drop=True)\n",
    "df_mut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "00b32cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Mutation Score: 55.84\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average Mutation Score: {df_mut['mutation_score'].mean():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

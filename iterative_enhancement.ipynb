{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b87c9697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pathlib\n",
    "from datasets import load_dataset\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b541135",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gemini-2.5-flash\"\n",
    "OUT_DIR = pathlib.Path(\"generated_solutions\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd0dc7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_docstring(prompt_text: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract all triple-quoted docstrings from the input text.\n",
    "    Returns a list of docstring contents as strings.\n",
    "    \"\"\"\n",
    "    matches = re.findall(r'(\"\"\"|\\'\\'\\')(.*?)(\\1)', prompt_text, flags=re.DOTALL)\n",
    "    # Join all docstrings into a single string (if needed)\n",
    "    doc_text = \"\\n\\n\".join([m[1].strip() for m in matches])\n",
    "\n",
    "    return doc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "229874a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HumanEval with 164 problems.\n"
     ]
    }
   ],
   "source": [
    "api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"Please set GOOGLE_API_KEY in your environment.\")\n",
    "genai.configure(api_key=api_key)\n",
    "model = genai.GenerativeModel(MODEL)\n",
    "\n",
    "ds = load_dataset(\"openai/openai_humaneval\")[\"test\"]  # 164 items\n",
    "print(f\"Loaded HumanEval with {len(ds)} problems.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e48d7531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation Complete\n"
     ]
    }
   ],
   "source": [
    "for idx, item in enumerate(ds):\n",
    "    doc = extract_docstring(item[\"prompt\"])\n",
    "    prompt = f'\"\"\"\\n{doc}\\n\"\"\"\\nPlease generate a plausible solution for this problem.'\n",
    "    # if(idx == 32):\n",
    "    #     print(prompt)\n",
    "    try:\n",
    "        resp = model.generate_content(prompt)\n",
    "        text = (resp.text or \"\").strip()\n",
    "        # If fences sneak in, strip them.\n",
    "        text = re.sub(r\"^```(?:python)?\\s*\", \"\", text)\n",
    "        text = re.sub(r\"\\s*```$\", \"\", text)\n",
    "\n",
    "        out_path = OUT_DIR / f\"Solution_{idx}.py\"\n",
    "        out_path.write_text(text, encoding=\"utf-8\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Problem {idx} failed: {e}\")\n",
    "        \n",
    "print(\"Generation Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25eeaa49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done. Scanned: 164 .py files; Modified: 0\n"
     ]
    }
   ],
   "source": [
    "# Remove everything above the first ```python fence (inclusive) in each .py file under DIR\n",
    "# Usage: set DIR below and run this cell.\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# <<< EDIT THIS >>>\n",
    "DIR = Path(\"generated_solutions\")  # e.g., Path(\"/home/jason/project\")\n",
    "# <<< ----------- >>>\n",
    "\n",
    "FENCE_RE = re.compile(r\"```[ \\t]*python\", re.IGNORECASE)\n",
    "\n",
    "def clean_file(p: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Find the first ```python fence in file p and remove everything above it\n",
    "    (including the fence itself). Returns True if the file was modified.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    except Exception as e:\n",
    "        print(f\"[skip read error] {p}: {e}\")\n",
    "        return False\n",
    "\n",
    "    m = FENCE_RE.search(text)\n",
    "    if not m:\n",
    "        return False  # no fence, leave file as-is\n",
    "\n",
    "    new_text = text[m.end():].lstrip(\"\\r\\n\")  # drop fence + any immediate blank lines\n",
    "    if new_text == text:\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        p.write_text(new_text, encoding=\"utf-8\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"[skip write error] {p}: {e}\")\n",
    "        return False\n",
    "\n",
    "def run(dir_path: Path):\n",
    "    if not dir_path.exists():\n",
    "        raise FileNotFoundError(f\"Directory not found: {dir_path}\")\n",
    "\n",
    "    py_files = [p for p in dir_path.rglob(\"*.py\") if p.is_file()]\n",
    "    changed = 0\n",
    "    for p in py_files:\n",
    "        if clean_file(p):\n",
    "            changed += 1\n",
    "            print(f\"[modified] {p}\")\n",
    "\n",
    "    print(f\"\\nDone. Scanned: {len(py_files)} .py files; Modified: {changed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc49069b",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c42803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "import os, re, io, sys, pathlib, types, importlib.util, builtins, json, textwrap\n",
    "import unittest\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "\n",
    "GEN_SOL_DIR = pathlib.Path(\"generated_solutions\").resolve()     # Solution_<i>.py\n",
    "TESTS_DIR   = pathlib.Path(\"generated_tests\").resolve()         # HumanEval_<i>.py\n",
    "CANON_DIR   = pathlib.Path(\"canonical_solutions\").resolve()     # 000_<entry>.py  (to infer entry_point)\n",
    "MAX_FIX_ATTEMPTS_PER_TEST = 3\n",
    "MODEL = \"gemini-2.5-flash\"\n",
    "\n",
    "assert GEN_SOL_DIR.is_dir(), f\"Missing {GEN_SOL_DIR}\"\n",
    "assert TESTS_DIR.is_dir(),   f\"Missing {TESTS_DIR}\"\n",
    "assert CANON_DIR.is_dir(),   f\"Missing {CANON_DIR}\"\n",
    "\n",
    "# --- Gemini client ---\n",
    "api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"Set GOOGLE_API_KEY before running.\")\n",
    "genai.configure(api_key=api_key)\n",
    "gmodel = genai.GenerativeModel(MODEL)\n",
    "\n",
    "# --- Map id -> entry_point from canonical_solutions (000_funcname.py)\n",
    "ID_TO_ENTRY = {}\n",
    "for f in sorted(CANON_DIR.glob(\"*.py\")):\n",
    "    m = re.match(r\"^(\\d{3})_(.+)\\.py$\", f.name)\n",
    "    if m:\n",
    "        ID_TO_ENTRY[int(m.group(1))] = m.group(2)\n",
    "\n",
    "# ========== Helpers ==========\n",
    "def read_text(p: pathlib.Path) -> str:\n",
    "    return p.read_text(encoding=\"utf-8\")\n",
    "\n",
    "def write_text(p: pathlib.Path, s: str):\n",
    "    p.write_text(s, encoding=\"utf-8\")\n",
    "\n",
    "def load_solution_module(sol_path: pathlib.Path) -> types.ModuleType:\n",
    "    \"\"\"Load generated solution into a fresh module, also expose as 'candidate'.\"\"\"\n",
    "    code = read_text(sol_path)\n",
    "    mod = types.ModuleType(\"candidate\")\n",
    "    exec(compile(code, str(sol_path), \"exec\"), mod.__dict__)\n",
    "    sys.modules[\"candidate\"] = mod\n",
    "    return mod\n",
    "\n",
    "def load_test_module_with_injection(test_path: pathlib.Path, entry_point: str, func_obj) -> types.ModuleType:\n",
    "    \"\"\"\n",
    "    Load test file as a fresh module and inject the function:\n",
    "    - in builtins (bare calls)\n",
    "    - in test module globals\n",
    "    - keep sys.modules clean between runs\n",
    "    \"\"\"\n",
    "    mod_name = f\"test_generated_{hash((str(test_path), os.urandom(4)))}\"\n",
    "    spec = importlib.util.spec_from_file_location(mod_name, str(test_path))\n",
    "    if spec is None or spec.loader is None:\n",
    "        raise RuntimeError(f\"Cannot load tests from {test_path}\")\n",
    "\n",
    "    tmod = importlib.util.module_from_spec(spec)\n",
    "    # injection\n",
    "    setattr(builtins, entry_point, func_obj)\n",
    "    tmod.__dict__[entry_point] = func_obj\n",
    "    sys.modules[mod_name] = tmod\n",
    "    spec.loader.exec_module(tmod)\n",
    "    return tmod\n",
    "\n",
    "def run_tests(test_module: types.ModuleType) -> unittest.TestResult:\n",
    "    stream = io.StringIO()\n",
    "    suite = unittest.defaultTestLoader.loadTestsFromModule(test_module)\n",
    "    runner = unittest.TextTestRunner(stream=stream, verbosity=0)\n",
    "    result = runner.run(suite)\n",
    "    result._stdout = stream.getvalue()\n",
    "    return result\n",
    "\n",
    "def extract_failed_tests(result: unittest.TestResult):\n",
    "    \"\"\"Return list of dicts: {'id': id_str, 'trace': tb_str} for failures+errors.\"\"\"\n",
    "    fails = []\n",
    "    for tc, tb in list(result.failures) + list(result.errors):\n",
    "        fails.append({\"id\": tc.id(), \"trace\": tb})\n",
    "    return fails\n",
    "\n",
    "def parse_test_id(test_id: str):\n",
    "    \"\"\"\n",
    "    unittest id() format: 'module.ClassName.test_method'\n",
    "    Returns (class_name, method_name).\n",
    "    \"\"\"\n",
    "    parts = test_id.split(\".\")\n",
    "    method = parts[-1]\n",
    "    cls = parts[-2] if len(parts) >= 2 else None\n",
    "    return cls, method\n",
    "\n",
    "def extract_test_method_code(test_path: pathlib.Path, class_name: str, method_name: str) -> tuple[str, tuple[int,int]]:\n",
    "    \"\"\"\n",
    "    Extract the exact text of a test method by scanning the file.\n",
    "    Returns (method_text, (start_idx, end_idx)) with indices into the file's lines list.\n",
    "    \"\"\"\n",
    "    src = read_text(test_path)\n",
    "    lines = src.splitlines(keepends=True)\n",
    "\n",
    "    # Find \"class ClassName(\" line\n",
    "    class_pat = re.compile(rf'^\\s*class\\s+{re.escape(class_name)}\\s*\\(', re.M)\n",
    "    class_match = None\n",
    "    for i, ln in enumerate(lines):\n",
    "        if class_pat.match(ln):\n",
    "            class_match = i\n",
    "            break\n",
    "    if class_match is None:\n",
    "        raise ValueError(f\"Class {class_name} not found in {test_path.name}\")\n",
    "\n",
    "    # Within class block, find 'def method_name(' at greater indent\n",
    "    # Determine class indent\n",
    "    class_indent = len(lines[class_match]) - len(lines[class_match].lstrip())\n",
    "    method_pat = re.compile(rf'^\\s*def\\s+{re.escape(method_name)}\\s*\\(', re.M)\n",
    "\n",
    "    start = end = None\n",
    "    for i in range(class_match + 1, len(lines)):\n",
    "        ln = lines[i]\n",
    "        indent = len(ln) - len(ln.lstrip())\n",
    "        if indent <= class_indent and ln.strip():  # class block ended\n",
    "            break\n",
    "        if start is None and method_pat.match(ln):\n",
    "            start = i\n",
    "            # capture until next def at same indent or class end\n",
    "            for j in range(i + 1, len(lines)):\n",
    "                ln2 = lines[j]\n",
    "                ind2 = len(ln2) - len(ln2.lstrip())\n",
    "                if re.match(r'^\\s*def\\s+\\w+\\s*\\(', ln2) and ind2 == indent:\n",
    "                    end = j\n",
    "                    break\n",
    "                # class end\n",
    "                if ind2 <= class_indent and ln2.strip():\n",
    "                    end = j\n",
    "                    break\n",
    "            if end is None:\n",
    "                end = len(lines)\n",
    "            break\n",
    "\n",
    "    if start is None:\n",
    "        raise ValueError(f\"Method {method_name} not found in {test_path.name}\")\n",
    "    method_text = \"\".join(lines[start:end])\n",
    "    return method_text, (start, end)\n",
    "\n",
    "def replace_block_in_file(test_path: pathlib.Path, span: tuple[int,int], new_block: str):\n",
    "    src = read_text(test_path)\n",
    "    lines = src.splitlines(keepends=True)\n",
    "    start, end = span\n",
    "    # Ensure trailing newline\n",
    "    if not new_block.endswith(\"\\n\"):\n",
    "        new_block += \"\\n\"\n",
    "    lines[start:end] = [new_block]\n",
    "    write_text(test_path, \"\".join(lines))\n",
    "\n",
    "def delete_block_in_file(test_path: pathlib.Path, span: tuple[int,int]):\n",
    "    replace_block_in_file(test_path, span, \"\")\n",
    "\n",
    "def prompt_gemini_fix(function_src: str, failing_test_src: str, error_text: str, method_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Ask Gemini to fix the failing test method. Returns ONLY the method code text.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are given a Python function under test and a single failing unittest method.\n",
    "Fix ONLY the test method so that it correctly tests the intended behavior of the function.\n",
    "\n",
    "Rules:\n",
    "- Do NOT change the function under test.\n",
    "- Keep it in Python's standard unittest style.\n",
    "- Prefer adjusting inputs/expected values or using correct assertions.\n",
    "- Keep the method name the same: {method_name}.\n",
    "- Return ONLY the updated method code that starts with `def {method_name}(` and ends at the end of the method. No extra text, no backticks, no comment explainations.\n",
    "\n",
    "Function under test:\n",
    "```python\n",
    "{function_src}\n",
    "```\n",
    "Failing test method:\n",
    "```python\n",
    "{failing_test_src}\n",
    "```\n",
    "Error message/trace:\n",
    "```python\n",
    "{error_text}\n",
    "```\n",
    "\"\"\"\n",
    "    resp = gmodel.generate_content(prompt)\n",
    "    text = (resp.text or \"\").strip()\n",
    "    # strip possible fences\n",
    "    text = re.sub(r\"^\\s*(?:python)?\\s*\", \"\", text) \n",
    "    text = re.sub(r\"```\", \"\", text)\n",
    "    text = re.sub(r\"\\s*\\s*$\", \"\", text)\n",
    "    print(text.strip())\n",
    "    return text.strip()\n",
    "\n",
    "def gather_function_source(sol_path: pathlib.Path, entry_point: str) -> str:\n",
    "    code = read_text(sol_path)\n",
    "    # Try to extract the function block; fallback to whole file\n",
    "    # This regex matches the function definition and its body (non-greedy, up to next def or end of file)\n",
    "    pattern = rf\"^def\\s+{re.escape(entry_point)}\\s*\\([^\\)]*\\):(?:\\n(?:[ \\t]+.*\\n?)*)*\"\n",
    "    m = re.search(pattern, code, flags=re.M)\n",
    "    if m:\n",
    "        return m.group(0)\n",
    "    return code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c08a043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== [000] has_close_elements ===\n",
      "  First run: 10/10 passed (0 failing)\n",
      "  Final: 10/10 passed (fixed 0, removed 0)\n",
      "\n",
      "=== [001] separate_paren_groups ===\n",
      "  First run: 10/10 passed (0 failing)\n",
      "  Final: 10/10 passed (fixed 0, removed 0)\n",
      "\n",
      "=== [002] truncate_number ===\n",
      "  First run: 10/10 passed (0 failing)\n",
      "  Final: 10/10 passed (fixed 0, removed 0)\n",
      "\n",
      "=== [003] below_zero ===\n",
      "  First run: 10/10 passed (0 failing)\n",
      "  Final: 10/10 passed (fixed 0, removed 0)\n",
      "\n",
      "=== [004] mean_absolute_deviation ===\n",
      "  First run: 10/10 passed (0 failing)\n",
      "  Final: 10/10 passed (fixed 0, removed 0)\n",
      "\n",
      "=== [005] intersperse ===\n",
      "  First run: 10/10 passed (0 failing)\n",
      "  Final: 10/10 passed (fixed 0, removed 0)\n",
      "\n",
      "=== [006] parse_nested_parens ===\n",
      "  First run: 9/10 passed (1 failing)\n",
      "    - Fixing test_very_deep_nesting_in_one_group (attempt 1)\n",
      "prompt:\n",
      " \n",
      "You are given a Python function under test and a single failing unittest method.\n",
      "Fix ONLY the test method so that it correctly tests the intended behavior of the function.\n",
      "\n",
      "Rules:\n",
      "- Do NOT change the function under test.\n",
      "- Keep it in Python's standard unittest style.\n",
      "- Prefer adjusting inputs/expected values or using correct assertions.\n",
      "- Keep the method name the same: test_very_deep_nesting_in_one_group.\n",
      "- Return ONLY the updated method code that starts with `def test_very_deep_nesting_in_one_group(` and ends at the end of the method. No extra text, no backticks, no comment explainations.\n",
      "\n",
      "Function under test:\n",
      "```python\n",
      "def parse_nested_parens(paren_string: str) -> list[int]:\n",
      "    \"\"\"\n",
      "    Input to this function is a string represented multiple groups for nested parentheses separated by spaces.\n",
      "    For each of the group, output the deepest level of nesting of parentheses.\n",
      "    E.g. (()()) has maximum two levels of nesting while ((())) has three.\n",
      "\n",
      "    >>> parse_nested_parens('(()()) ((())) () ((())()())')\n",
      "    [2, 3, 1, 3]\n",
      "    >>> parse_nested_parens('()')\n",
      "    [1]\n",
      "    >>> parse_nested_parens('((()))')\n",
      "    [3]\n",
      "    >>> parse_nested_parens('')\n",
      "    []\n",
      "    >>> parse_nested_parens('()()()')\n",
      "    [1, 1, 1]\n",
      "    >>> parse_nested_parens('((())) (()) ()')\n",
      "    [3, 2, 1]\n",
      "    \"\"\"\n",
      "    results = []\n",
      "    \n",
      "    # Split the input string into individual groups of parentheses.\n",
      "    # Using .split() without arguments handles multiple spaces and leading/trailing spaces correctly.\n",
      "    groups = paren_string.split() \n",
      "\n",
      "    for group in groups:\n",
      "        current_depth = 0  # Tracks the current nesting level\n",
      "        max_depth = 0      # Stores the maximum nesting level found so far for the current group\n",
      "\n",
      "        for char in group:\n",
      "            if char == '(':\n",
      "                current_depth += 1\n",
      "                # Update max_depth if the current_depth is greater\n",
      "                max_depth = max(max_depth, current_depth)\n",
      "            elif char == ')':\n",
      "                # For a closing parenthesis, we simply decrease the current_depth.\n",
      "                # We don't update max_depth here because deeper nesting is found with '('.\n",
      "                current_depth -= 1\n",
      "        \n",
      "        results.append(max_depth)\n",
      "        \n",
      "    return results\n",
      "```\n",
      "Failing test method:\n",
      "```python\n",
      "    def test_very_deep_nesting_in_one_group(self):\n",
      "        \"\"\"Test a single group with an exceptionally deep level of nesting.\"\"\"\n",
      "        self.assertEqual(parse_nested_parens('((((((()))))))'), [6])\n",
      "\n",
      "\n",
      "```\n",
      "Error message/trace:\n",
      "```python\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\zhang\\Downloads\\Evaluating_and_Enhancing_LLM_Generated_Test_Suites\\generated_tests\\HumanEval_6.py\", line 62, in test_very_deep_nesting_in_one_group\n",
      "    self.assertEqual(parse_nested_parens('((((((()))))))'), [6])\n",
      "AssertionError: Lists differ: [7] != [6]\n",
      "\n",
      "First differing element 0:\n",
      "7\n",
      "6\n",
      "\n",
      "- [7]\n",
      "+ [6]\n",
      "\n",
      "```\n",
      "\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (HumanEval_6.py, line 63)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3577\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[33], line 64\u001b[0m\n    tmod = load_test_module_with_injection(test_file, entry_point, getattr(func_mod, entry_point))\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[28], line 63\u001b[0m in \u001b[0;35mload_test_module_with_injection\u001b[0m\n    spec.loader.exec_module(tmod)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m<frozen importlib._bootstrap_external>:991\u001b[0m in \u001b[0;35mexec_module\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m<frozen importlib._bootstrap_external>:1129\u001b[0m in \u001b[0;35mget_code\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m<frozen importlib._bootstrap_external>:1059\u001b[0m in \u001b[0;35msource_to_code\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[1;36m in \u001b[1;35m_call_with_frames_removed\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32m~\\Downloads\\Evaluating_and_Enhancing_LLM_Generated_Test_Suites\\generated_tests\\HumanEval_6.py:63\u001b[1;36m\u001b[0m\n\u001b[1;33m    ```\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# ========== Main loop ==========\n",
    "rows = []\n",
    "for sol_file in sorted(GEN_SOL_DIR.glob(\"Solution_*.py\"), key=lambda p: int(re.search(r\"(\\d+)\", p.stem).group(1))):\n",
    "    idx = int(re.search(r\"(\\d+)\", sol_file.stem).group(1))\n",
    "    test_file = TESTS_DIR / f\"HumanEval_{idx}.py\"\n",
    "    if not test_file.exists():\n",
    "        print(f\"[SKIP] {idx:03d}: no test file {test_file.name}\")\n",
    "        continue\n",
    "    if idx not in ID_TO_ENTRY:\n",
    "        print(f\"[SKIP] {idx:03d}: no entry_point mapping from canonical filenames.\")\n",
    "        continue\n",
    "    entry_point = ID_TO_ENTRY[idx]\n",
    "    print(f\"\\n=== [{idx:03d}] {entry_point} ===\")\n",
    "\n",
    "    original_test_src = read_text(test_file)  # keep original for span math\n",
    "    func_mod = load_solution_module(sol_file)\n",
    "    if not hasattr(func_mod, entry_point):\n",
    "        print(f\"  [WARN] Function '{entry_point}' not found in {sol_file.name}; skipping.\")\n",
    "        rows.append({\"id\": idx, \"entry_point\": entry_point, \"initial_failures\": None, \"fixed\": 0, \"removed\": 0, \"final_passed\": 0, \"final_total\": 0})\n",
    "        continue\n",
    "\n",
    "    # -- First run --\n",
    "    tmod = load_test_module_with_injection(test_file, entry_point, getattr(func_mod, entry_point))\n",
    "    result = run_tests(tmod)\n",
    "    total = result.testsRun\n",
    "    fails = extract_failed_tests(result)\n",
    "    print(f\"  First run: {total - len(fails)}/{total} passed ({len(fails)} failing)\")\n",
    "\n",
    "    fixed_count = 0\n",
    "    removed_count = 0\n",
    "\n",
    "    # Feedback loop: fix each failing test method up to 3 attempts; if still failing -> delete\n",
    "    for fail in fails:\n",
    "        cls_name, meth_name = parse_test_id(fail[\"id\"])\n",
    "        if not cls_name or not meth_name:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            failing_src, span = extract_test_method_code(test_file, cls_name, meth_name)\n",
    "        except Exception as e:\n",
    "            print(f\"  [WARN] Cannot extract test {meth_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        func_src = gather_function_source(sol_file, entry_point)\n",
    "        attempt = 0\n",
    "        success = False\n",
    "        while attempt < MAX_FIX_ATTEMPTS_PER_TEST:\n",
    "            attempt += 1\n",
    "            print(f\"    - Fixing {meth_name} (attempt {attempt})\")\n",
    "            try:\n",
    "                new_method = prompt_gemini_fix(func_src, failing_src, fail[\"trace\"], meth_name)\n",
    "                if not new_method.strip().startswith(f\"def {meth_name}(\"):\n",
    "                    # If model changed the name or wrapped content, try to salvage by regex\n",
    "                    m = re.search(rf'^\\s*def\\s+{re.escape(meth_name)}\\s*\\(.*', new_method, flags=re.M)\n",
    "                    if m:\n",
    "                        new_method = new_method[m.start():]\n",
    "                replace_block_in_file(test_file, span, new_method)\n",
    "            except Exception as e:\n",
    "                print(f\"      [LLM/Patch error] {e}\")\n",
    "                break\n",
    "\n",
    "            # Re-run tests after patch\n",
    "            func_mod = load_solution_module(sol_file)  # reload solution into 'candidate'\n",
    "            tmod = load_test_module_with_injection(test_file, entry_point, getattr(func_mod, entry_point))\n",
    "            res2 = run_tests(tmod)\n",
    "            current_fails = extract_failed_tests(res2)\n",
    "            still_failing_ids = {f[\"id\"] for f in current_fails}\n",
    "            if fail[\"id\"] not in still_failing_ids:\n",
    "                print(\"      ✓ fixed\")\n",
    "                fixed_count += 1\n",
    "                success = True\n",
    "                # refresh original_test_src and span indices for further patches in same file\n",
    "                original_test_src = read_text(test_file)\n",
    "                # recompute span for safety if another failure in same method appears (rare)\n",
    "                break\n",
    "            else:\n",
    "                # update failing_src / span from the latest test file (may have changed formatting)\n",
    "                try:\n",
    "                    failing_src, span = extract_test_method_code(test_file, cls_name, meth_name)\n",
    "                except Exception:\n",
    "                    pass  # keep previous span\n",
    "\n",
    "        if not success:\n",
    "            print(\"      ✗ still failing after 3 attempts → deleting test\")\n",
    "            delete_block_in_file(test_file, span)\n",
    "            removed_count += 1\n",
    "\n",
    "    # Final run after all fixes/removals\n",
    "    func_mod = load_solution_module(sol_file)\n",
    "    tmod = load_test_module_with_injection(test_file, entry_point, getattr(func_mod, entry_point))\n",
    "    final_res = run_tests(tmod)\n",
    "    final_total = final_res.testsRun\n",
    "    final_fails = len(extract_failed_tests(final_res))\n",
    "    final_passed = final_total - final_fails\n",
    "    print(f\"  Final: {final_passed}/{final_total} passed (fixed {fixed_count}, removed {removed_count})\")\n",
    "\n",
    "    rows.append({\n",
    "        \"id\": idx,\n",
    "        \"entry_point\": entry_point,\n",
    "        \"initial_failures\": len(fails),\n",
    "        \"fixed\": fixed_count,\n",
    "        \"removed\": removed_count,\n",
    "        \"final_passed\": final_passed,\n",
    "        \"final_total\": final_total,\n",
    "    })\n",
    "    if idx == 6:\n",
    "        break  # DEBUG\n",
    "\n",
    "df_fix = pd.DataFrame(rows).sort_values(\"id\").reset_index(drop=True)\n",
    "df_fix\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

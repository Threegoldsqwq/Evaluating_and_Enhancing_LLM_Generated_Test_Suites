{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b87c9697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pathlib\n",
    "from datasets import load_dataset\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b541135",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gemini-2.5-flash\"\n",
    "OUT_DIR = pathlib.Path(\"generated_solutions\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd0dc7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_docstring(prompt_text: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract all triple-quoted docstrings from the input text.\n",
    "    Returns a list of docstring contents as strings.\n",
    "    \"\"\"\n",
    "    matches = re.findall(r'(\"\"\"|\\'\\'\\')(.*?)(\\1)', prompt_text, flags=re.DOTALL)\n",
    "    # Join all docstrings into a single string (if needed)\n",
    "    doc_text = \"\\n\\n\".join([m[1].strip() for m in matches])\n",
    "\n",
    "    return doc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "229874a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HumanEval with 164 problems.\n"
     ]
    }
   ],
   "source": [
    "api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"Please set GOOGLE_API_KEY in your environment.\")\n",
    "genai.configure(api_key=api_key)\n",
    "model = genai.GenerativeModel(MODEL)\n",
    "\n",
    "ds = load_dataset(\"openai/openai_humaneval\")[\"test\"]  # 164 items\n",
    "print(f\"Loaded HumanEval with {len(ds)} problems.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e48d7531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation Complete\n"
     ]
    }
   ],
   "source": [
    "for idx, item in enumerate(ds):\n",
    "    doc = extract_docstring(item[\"prompt\"])\n",
    "    prompt = f'\"\"\"\\n{doc}\\n\"\"\"\\nPlease generate a plausible solution for this problem.'\n",
    "    # if(idx == 32):\n",
    "    #     print(prompt)\n",
    "    try:\n",
    "        resp = model.generate_content(prompt)\n",
    "        text = (resp.text or \"\").strip()\n",
    "        # If fences sneak in, strip them.\n",
    "        text = re.sub(r\"^```(?:python)?\\s*\", \"\", text)\n",
    "        text = re.sub(r\"\\s*```$\", \"\", text)\n",
    "\n",
    "        out_path = OUT_DIR / f\"Solution_{idx}.py\"\n",
    "        out_path.write_text(text, encoding=\"utf-8\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Problem {idx} failed: {e}\")\n",
    "        \n",
    "print(\"Generation Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25eeaa49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done. Scanned: 164 .py files; Modified: 0\n"
     ]
    }
   ],
   "source": [
    "# Remove everything above the first ```python fence (inclusive) in each .py file under DIR\n",
    "# Usage: set DIR below and run this cell.\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# <<< EDIT THIS >>>\n",
    "DIR = Path(\"generated_solutions\")  # e.g., Path(\"/home/jason/project\")\n",
    "# <<< ----------- >>>\n",
    "\n",
    "FENCE_RE = re.compile(r\"```[ \\t]*python\", re.IGNORECASE)\n",
    "\n",
    "def clean_file(p: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Find the first ```python fence in file p and remove everything above it\n",
    "    (including the fence itself). Returns True if the file was modified.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    except Exception as e:\n",
    "        print(f\"[skip read error] {p}: {e}\")\n",
    "        return False\n",
    "\n",
    "    m = FENCE_RE.search(text)\n",
    "    if not m:\n",
    "        return False  # no fence, leave file as-is\n",
    "\n",
    "    new_text = text[m.end():].lstrip(\"\\r\\n\")  # drop fence + any immediate blank lines\n",
    "    if new_text == text:\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        p.write_text(new_text, encoding=\"utf-8\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"[skip write error] {p}: {e}\")\n",
    "        return False\n",
    "\n",
    "def run(dir_path: Path):\n",
    "    if not dir_path.exists():\n",
    "        raise FileNotFoundError(f\"Directory not found: {dir_path}\")\n",
    "\n",
    "    py_files = [p for p in dir_path.rglob(\"*.py\") if p.is_file()]\n",
    "    changed = 0\n",
    "    for p in py_files:\n",
    "        if clean_file(p):\n",
    "            changed += 1\n",
    "            print(f\"[modified] {p}\")\n",
    "\n",
    "    print(f\"\\nDone. Scanned: {len(py_files)} .py files; Modified: {changed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc49069b",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "66c42803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "import os, re, io, sys, pathlib, types, importlib.util, builtins, json, textwrap\n",
    "import unittest\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "\n",
    "GEN_SOL_DIR = pathlib.Path(\"generated_solutions\").resolve()     # Solution_<i>.py\n",
    "TESTS_DIR   = pathlib.Path(\"generated_tests\").resolve()         # HumanEval_<i>.py\n",
    "CANON_DIR   = pathlib.Path(\"canonical_solutions\").resolve()     # 000_<entry>.py  (to infer entry_point)\n",
    "MAX_FIX_ATTEMPTS_PER_TEST = 3\n",
    "MODEL = \"gemini-2.5-flash\"\n",
    "\n",
    "assert GEN_SOL_DIR.is_dir(), f\"Missing {GEN_SOL_DIR}\"\n",
    "assert TESTS_DIR.is_dir(),   f\"Missing {TESTS_DIR}\"\n",
    "assert CANON_DIR.is_dir(),   f\"Missing {CANON_DIR}\"\n",
    "\n",
    "# --- Gemini client ---\n",
    "api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"Set GOOGLE_API_KEY before running.\")\n",
    "genai.configure(api_key=api_key)\n",
    "gmodel = genai.GenerativeModel(MODEL)\n",
    "\n",
    "# --- Map id -> entry_point from canonical_solutions (000_funcname.py)\n",
    "ID_TO_ENTRY = {}\n",
    "for f in sorted(CANON_DIR.glob(\"*.py\")):\n",
    "    m = re.match(r\"^(\\d{3})_(.+)\\.py$\", f.name)\n",
    "    if m:\n",
    "        ID_TO_ENTRY[int(m.group(1))] = m.group(2)\n",
    "\n",
    "# ========== Helpers ==========\n",
    "def read_text(p: pathlib.Path) -> str:\n",
    "    return p.read_text(encoding=\"utf-8\")\n",
    "\n",
    "def write_text(p: pathlib.Path, s: str):\n",
    "    p.write_text(s, encoding=\"utf-8\")\n",
    "\n",
    "def load_solution_module(sol_path: pathlib.Path) -> types.ModuleType:\n",
    "    \"\"\"Load generated solution into a fresh module, also expose as 'candidate'.\"\"\"\n",
    "    code = read_text(sol_path)\n",
    "    mod = types.ModuleType(\"candidate\")\n",
    "    exec(compile(code, str(sol_path), \"exec\"), mod.__dict__)\n",
    "    sys.modules[\"candidate\"] = mod\n",
    "    return mod\n",
    "\n",
    "def load_test_module_with_injection(test_path: pathlib.Path, entry_point: str, func_obj) -> types.ModuleType:\n",
    "    \"\"\"\n",
    "    Load test file as a fresh module and inject the function:\n",
    "    - in builtins (bare calls)\n",
    "    - in test module globals\n",
    "    - keep sys.modules clean between runs\n",
    "    \"\"\"\n",
    "    mod_name = f\"test_generated_{hash((str(test_path), os.urandom(4)))}\"\n",
    "    spec = importlib.util.spec_from_file_location(mod_name, str(test_path))\n",
    "    if spec is None or spec.loader is None:\n",
    "        raise RuntimeError(f\"Cannot load tests from {test_path}\")\n",
    "\n",
    "    tmod = importlib.util.module_from_spec(spec)\n",
    "    # injection\n",
    "    setattr(builtins, entry_point, func_obj)\n",
    "    tmod.__dict__[entry_point] = func_obj\n",
    "    sys.modules[mod_name] = tmod\n",
    "    spec.loader.exec_module(tmod)\n",
    "    return tmod\n",
    "\n",
    "def run_tests(test_module: types.ModuleType) -> unittest.TestResult:\n",
    "    stream = io.StringIO()\n",
    "    suite = unittest.defaultTestLoader.loadTestsFromModule(test_module)\n",
    "    runner = unittest.TextTestRunner(stream=stream, verbosity=0)\n",
    "    result = runner.run(suite)\n",
    "    result._stdout = stream.getvalue()\n",
    "    return result\n",
    "\n",
    "def extract_failed_tests(result: unittest.TestResult):\n",
    "    \"\"\"Return list of dicts: {'id': id_str, 'trace': tb_str} for failures+errors.\"\"\"\n",
    "    fails = []\n",
    "    for tc, tb in list(result.failures) + list(result.errors):\n",
    "        fails.append({\"id\": tc.id(), \"trace\": tb})\n",
    "    return fails\n",
    "\n",
    "def parse_test_id(test_id: str):\n",
    "    \"\"\"\n",
    "    unittest id() format: 'module.ClassName.test_method'\n",
    "    Returns (class_name, method_name).\n",
    "    \"\"\"\n",
    "    parts = test_id.split(\".\")\n",
    "    method = parts[-1]\n",
    "    cls = parts[-2] if len(parts) >= 2 else None\n",
    "    return cls, method\n",
    "\n",
    "def extract_test_method_code(test_path: pathlib.Path, class_name: str, method_name: str) -> tuple[str, tuple[int,int]]:\n",
    "    \"\"\"\n",
    "    Extract the exact text of a test method by scanning the file.\n",
    "    Returns (method_text, (start_idx, end_idx)) with indices into the file's lines list.\n",
    "    \"\"\"\n",
    "    src = read_text(test_path)\n",
    "    lines = src.splitlines(keepends=True)\n",
    "\n",
    "    # Find \"class ClassName(\" line\n",
    "    class_pat = re.compile(rf'^\\s*class\\s+{re.escape(class_name)}\\s*\\(', re.M)\n",
    "    class_match = None\n",
    "    for i, ln in enumerate(lines):\n",
    "        if class_pat.match(ln):\n",
    "            class_match = i\n",
    "            break\n",
    "    if class_match is None:\n",
    "        raise ValueError(f\"Class {class_name} not found in {test_path.name}\")\n",
    "\n",
    "    # Within class block, find 'def method_name(' at greater indent\n",
    "    # Determine class indent\n",
    "    class_indent = len(lines[class_match]) - len(lines[class_match].lstrip())\n",
    "    method_pat = re.compile(rf'^\\s*def\\s+{re.escape(method_name)}\\s*\\(', re.M)\n",
    "\n",
    "    start = end = None\n",
    "    for i in range(class_match + 1, len(lines)):\n",
    "        ln = lines[i]\n",
    "        indent = len(ln) - len(ln.lstrip())\n",
    "        if indent <= class_indent and ln.strip():  # class block ended\n",
    "            break\n",
    "        if start is None and method_pat.match(ln):\n",
    "            start = i\n",
    "            # capture until next def at same indent or class end\n",
    "            for j in range(i + 1, len(lines)):\n",
    "                ln2 = lines[j]\n",
    "                ind2 = len(ln2) - len(ln2.lstrip())\n",
    "                if re.match(r'^\\s*def\\s+\\w+\\s*\\(', ln2) and ind2 == indent:\n",
    "                    end = j\n",
    "                    break\n",
    "                # class end\n",
    "                if ind2 <= class_indent and ln2.strip():\n",
    "                    end = j\n",
    "                    break\n",
    "            if end is None:\n",
    "                end = len(lines)\n",
    "            break\n",
    "\n",
    "    if start is None:\n",
    "        raise ValueError(f\"Method {method_name} not found in {test_path.name}\")\n",
    "    method_text = \"\".join(lines[start:end])\n",
    "    return method_text, (start, end)\n",
    "\n",
    "def replace_block_in_file(test_path: pathlib.Path, span: tuple[int,int], new_block: str):\n",
    "    src = read_text(test_path)\n",
    "    lines = src.splitlines(keepends=True)\n",
    "    start, end = span\n",
    "    # Ensure trailing newline\n",
    "    if not new_block.endswith(\"\\n\"):\n",
    "        new_block += \"\\n\"\n",
    "    lines[start:end] = [new_block]\n",
    "    write_text(test_path, \"\".join(lines))\n",
    "\n",
    "def delete_block_in_file(test_path: pathlib.Path, span: tuple[int,int]):\n",
    "    replace_block_in_file(test_path, span, \"\")\n",
    "\n",
    "import re, textwrap\n",
    "\n",
    "def clean_and_align_method_code(raw_text: str, target_indent: str, method_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize a test method snippet and make sure the *first def line* is indented\n",
    "    to `target_indent`, with the body keeping its relative indentation.\n",
    "    \"\"\"\n",
    "    # remove code fences / normalize whitespace\n",
    "    text = (raw_text or \"\")\n",
    "    text = re.sub(r\"```(?:python)?\", \"\", text)\n",
    "    text = re.sub(r\"```\", \"\", text)\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\").expandtabs(4).strip()\n",
    "\n",
    "    # keep only the requested method if extra prose is present\n",
    "    m = re.search(rf'^\\s*def\\s+{re.escape(method_name)}\\s*\\(.*', text, flags=re.M)\n",
    "    if m:\n",
    "        text = text[m.start():]\n",
    "\n",
    "    # dedent everything so relative body indentation is preserved\n",
    "    text = textwrap.dedent(text).strip(\"\\n\")\n",
    "\n",
    "    # split, then force-indent the first def line\n",
    "    lines = text.split(\"\\n\")\n",
    "    if lines:\n",
    "        lines[0] = target_indent + lines[0].lstrip()  # <-- ensure the first def is indented\n",
    "\n",
    "    # indent all remaining lines by target_indent too (keeps relative indents from dedent)\n",
    "    for i in range(1, len(lines)):\n",
    "        if lines[i].strip():\n",
    "            lines[i] = target_indent + lines[i]\n",
    "\n",
    "    out = \"\\n\".join(lines)\n",
    "    if not out.endswith(\"\\n\"):\n",
    "        out += \"\\n\"\n",
    "    return out\n",
    "\n",
    "\n",
    "def prompt_gemini_fix(function_src: str, failing_test_src: str, error_text: str, method_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Ask Gemini to fix the failing test method. Returns ONLY the method code text.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are given a Python function under test and a single failing unittest method.\n",
    "Fix ONLY the test method so that it correctly tests the intended behavior of the function.\n",
    "\n",
    "Rules:\n",
    "- Do NOT change the function under test.\n",
    "- Keep it in Python's standard unittest style.\n",
    "- Prefer adjusting inputs/expected values or using correct assertions.\n",
    "- Keep the method name the same: {method_name}.\n",
    "- Return ONLY the updated method code that starts with `def {method_name}(` and ends at the end of the method. No extra text, no backticks, no comment explainations.\n",
    "\n",
    "Function under test:\n",
    "```python\n",
    "{function_src}\n",
    "```\n",
    "Failing test method:\n",
    "```python\n",
    "{failing_test_src}\n",
    "```\n",
    "Error message/trace:\n",
    "```python\n",
    "{error_text}\n",
    "```\n",
    "\"\"\"\n",
    "    resp = gmodel.generate_content(prompt)\n",
    "    text = (resp.text or \"\").strip()\n",
    "    # strip possible fences\n",
    "    text = re.sub(r\"^\\s*(?:python)?\\s*\", \"\", text) \n",
    "    text = re.sub(r\"```\", \"\", text)\n",
    "    text = re.sub(r\"\\s*\\s*$\", \"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def gather_function_source(sol_path: pathlib.Path, entry_point: str) -> str:\n",
    "    code = read_text(sol_path)\n",
    "    # Try to extract the function block; fallback to whole file\n",
    "    # This regex matches the function definition and its body (non-greedy, up to next def or end of file)\n",
    "    pattern = rf\"^def\\s+{re.escape(entry_point)}\\s*\\([^\\)]*\\):(?:\\n(?:[ \\t]+.*\\n?)*)*\"\n",
    "    m = re.search(pattern, code, flags=re.M)\n",
    "    if m:\n",
    "        return m.group(0)\n",
    "    return code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c08a043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== [147] get_max_triples ===\n",
      "  First run: 0/8 passed (8 failing)\n",
      "    - Fixing test_n_100_medium_large (attempt 1)\n",
      "      ✓ fixed\n",
      "    - Fixing test_n_2 (attempt 1)\n",
      "      ✓ fixed\n",
      "    - Fixing test_n_3 (attempt 1)\n",
      "      ✓ fixed\n",
      "    - Fixing test_n_4 (attempt 1)\n",
      "      ✓ fixed\n",
      "    - Fixing test_n_5_example (attempt 1)\n",
      "      ✓ fixed\n",
      "    - Fixing test_n_6 (attempt 1)\n",
      "      ✓ fixed\n",
      "    - Fixing test_n_8 (attempt 1)\n",
      "      ✓ fixed\n",
      "    - Fixing test_n_9 (attempt 1)\n",
      "      ✓ fixed\n",
      "  Final: 6/8 passed (fixed 8, removed 0)\n"
     ]
    }
   ],
   "source": [
    "# ========== Main loop ==========\n",
    "rows = []\n",
    "for sol_file in sorted(GEN_SOL_DIR.glob(\"Solution_*.py\"), key=lambda p: int(re.search(r\"(\\d+)\", p.stem).group(1))):\n",
    "\n",
    "    idx = int(re.search(r\"(\\d+)\", sol_file.stem).group(1))\n",
    "    # if (idx != 147):\n",
    "    #     continue # DEBUG\n",
    "    test_file = TESTS_DIR / f\"HumanEval_{idx}.py\"\n",
    "    if not test_file.exists():\n",
    "        print(f\"[SKIP] {idx:03d}: no test file {test_file.name}\")\n",
    "        continue\n",
    "    if idx not in ID_TO_ENTRY:\n",
    "        print(f\"[SKIP] {idx:03d}: no entry_point mapping from canonical filenames.\")\n",
    "        continue\n",
    "    entry_point = ID_TO_ENTRY[idx]\n",
    "    print(f\"\\n=== [{idx:03d}] {entry_point} ===\")\n",
    "\n",
    "    original_test_src = read_text(test_file)  # keep original for span math\n",
    "    func_mod = load_solution_module(sol_file)\n",
    "    if not hasattr(func_mod, entry_point):\n",
    "        print(f\"  [WARN] Function '{entry_point}' not found in {sol_file.name}; skipping.\")\n",
    "        rows.append({\"id\": idx, \"entry_point\": entry_point, \"initial_failures\": None, \"fixed\": 0, \"removed\": 0, \"final_passed\": 0, \"final_total\": 0})\n",
    "        continue\n",
    "    \n",
    "    # -- First run --\n",
    "    tmod = load_test_module_with_injection(test_file, entry_point, getattr(func_mod, entry_point))\n",
    "    result = run_tests(tmod)\n",
    "    total = result.testsRun\n",
    "    fails = extract_failed_tests(result)\n",
    "    print(f\"  First run: {total - len(fails)}/{total} passed ({len(fails)} failing)\")\n",
    "\n",
    "    fixed_count = 0\n",
    "    removed_count = 0\n",
    "\n",
    "    # Feedback loop: fix each failing test method up to 3 attempts; if still failing -> delete\n",
    "    for fail in fails:\n",
    "        cls_name, meth_name = parse_test_id(fail[\"id\"])\n",
    "        if not cls_name or not meth_name:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            failing_src, span = extract_test_method_code(test_file, cls_name, meth_name)\n",
    "        except Exception as e:\n",
    "            print(f\"  [WARN] Cannot extract test {meth_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        func_src = gather_function_source(sol_file, entry_point)\n",
    "        attempt = 0\n",
    "        success = False\n",
    "        while attempt < MAX_FIX_ATTEMPTS_PER_TEST:\n",
    "            attempt += 1\n",
    "            print(f\"    - Fixing {meth_name} (attempt {attempt})\")\n",
    "            try:\n",
    "                new_method = prompt_gemini_fix(func_src, failing_src, fail[\"trace\"], meth_name)\n",
    "                if not new_method.strip().startswith(f\"def {meth_name}(\"):\n",
    "                    # If model changed the name or wrapped content, try to salvage by regex\n",
    "                    m = re.search(rf'^\\s*def\\s+{re.escape(meth_name)}\\s*\\(.*', new_method, flags=re.M)\n",
    "                    if m:\n",
    "                        new_method = new_method[m.start():]\n",
    "\n",
    "                original_src_lines = read_text(test_file).splitlines(keepends=True)\n",
    "                target_indent = re.match(r'^(\\s*)', original_src_lines[span[0]]).group(1)\n",
    "                new_method_aligned = clean_and_align_method_code(new_method, target_indent, meth_name)\n",
    "                replace_block_in_file(test_file, span, new_method_aligned)\n",
    "            except Exception as e:\n",
    "                print(f\"      [LLM/Patch error] {e}\")\n",
    "                break\n",
    "\n",
    "            # Re-run tests after patch\n",
    "            func_mod = load_solution_module(sol_file)  # reload solution into 'candidate'\n",
    "            tmod = load_test_module_with_injection(test_file, entry_point, getattr(func_mod, entry_point))\n",
    "            res2 = run_tests(tmod)\n",
    "            current_fails = extract_failed_tests(res2)\n",
    "            still_failing_ids = {f[\"id\"] for f in current_fails}\n",
    "            if fail[\"id\"] not in still_failing_ids:\n",
    "                print(\"      ✓ fixed\")\n",
    "                fixed_count += 1\n",
    "                success = True\n",
    "                # refresh original_test_src and span indices for further patches in same file\n",
    "                original_test_src = read_text(test_file)\n",
    "                # recompute span for safety if another failure in same method appears (rare)\n",
    "                break\n",
    "            else:\n",
    "                # update failing_src / span from the latest test file (may have changed formatting)\n",
    "                try:\n",
    "                    failing_src, span = extract_test_method_code(test_file, cls_name, meth_name)\n",
    "                except Exception:\n",
    "                    pass  # keep previous span\n",
    "\n",
    "        if not success:\n",
    "            print(\"      ✗ still failing after 3 attempts → deleting test\")\n",
    "            delete_block_in_file(test_file, span)\n",
    "            removed_count += 1\n",
    "\n",
    "    # Final run after all fixes/removals\n",
    "    func_mod = load_solution_module(sol_file)\n",
    "    tmod = load_test_module_with_injection(test_file, entry_point, getattr(func_mod, entry_point))\n",
    "    final_res = run_tests(tmod)\n",
    "    final_total = final_res.testsRun\n",
    "    final_fails = len(extract_failed_tests(final_res))\n",
    "    final_passed = final_total - final_fails\n",
    "    print(f\"  Final: {final_passed}/{final_total} passed (fixed {fixed_count}, removed {removed_count})\")\n",
    "\n",
    "    rows.append({\n",
    "        \"id\": idx,\n",
    "        \"entry_point\": entry_point,\n",
    "        \"initial_failures\": len(fails),\n",
    "        \"fixed\": fixed_count,\n",
    "        \"removed\": removed_count,\n",
    "        \"final_passed\": final_passed,\n",
    "        \"final_total\": final_total,\n",
    "    })\n",
    "    # if idx == 7:\n",
    "    #     break  # DEBUG\n",
    "\n",
    "df_fix = pd.DataFrame(rows).sort_values(\"id\").reset_index(drop=True)\n",
    "df_fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078d1d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fix.to_csv(\"C:\\\\Users\\\\zhang\\\\Downloads\\\\fix.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edf1873",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "18c041c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins, importlib.util, io, pathlib, re, sys, types, unittest, warnings, textwrap\n",
    "import pandas as pd\n",
    "from coverage import Coverage\n",
    "from coverage.exceptions import CoverageWarning\n",
    "\n",
    "# Paths\n",
    "GEN_SOL_DIR = pathlib.Path(\"generated_solutions\").resolve()\n",
    "TESTS_DIR   = pathlib.Path(\"generated_tests\").resolve()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=CoverageWarning)\n",
    "\n",
    "def read_text(p: pathlib.Path) -> str:\n",
    "    return p.read_text(encoding=\"utf-8\")\n",
    "\n",
    "def load_solution_as_module(sol_path: pathlib.Path) -> types.ModuleType:\n",
    "    code = read_text(sol_path)\n",
    "    mod = types.ModuleType(\"candidate\")\n",
    "    compiled = compile(code, filename=str(sol_path), mode=\"exec\")\n",
    "    exec(compiled, mod.__dict__)\n",
    "    return mod\n",
    "\n",
    "def infer_entry_point_from_solution(code: str) -> str:\n",
    "    m = re.search(r'^\\s*def\\s+([A-Za-z_]\\w*)\\s*\\(', code, flags=re.M)\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "def load_test_module(test_path: pathlib.Path, entry_point_name: str, func_obj) -> types.ModuleType:\n",
    "    setattr(builtins, entry_point_name, func_obj)\n",
    "    spec = importlib.util.spec_from_file_location(\"test_generated\", str(test_path))\n",
    "    tmod = importlib.util.module_from_spec(spec)\n",
    "    tmod.__dict__[entry_point_name] = func_obj\n",
    "    sys.modules[\"test_generated\"] = tmod\n",
    "    spec.loader.exec_module(tmod)\n",
    "    return tmod\n",
    "\n",
    "def run_suite_under_coverage(sol_path: pathlib.Path, suite: unittest.TestSuite):\n",
    "    cov = Coverage(include=[str(sol_path)], branch=True)\n",
    "    cov.erase()\n",
    "    cov.start()\n",
    "    result = unittest.TextTestRunner(stream=io.StringIO(), verbosity=0).run(suite)\n",
    "    cov.stop()\n",
    "    cov.save()\n",
    "    return cov, result\n",
    "\n",
    "def coverage_metrics_for_file(cov: Coverage, filename: str):\n",
    "    try:\n",
    "        data = cov.get_data()\n",
    "        if not data:\n",
    "            return 0.0, [], [], []\n",
    "        measured = data.measured_files()\n",
    "        if filename not in measured:\n",
    "            for f in measured:\n",
    "                if pathlib.Path(f).name == pathlib.Path(filename).name:\n",
    "                    filename = f\n",
    "                    break\n",
    "            else:\n",
    "                return 0.0, [], [], []\n",
    "        _, statements, _, missing, partial = cov.analysis2(filename)\n",
    "        pct = 0.0 if not statements else round(100.0 * (len(statements) - len(missing)) / len(statements), 2)\n",
    "        return pct, statements, missing, partial\n",
    "    except Exception:\n",
    "        return 0.0, [], [], []\n",
    "\n",
    "def make_augmentation_prompt(solution_code: str, missing_lines, partial_lines) -> str:\n",
    "    feedback = []\n",
    "    if missing_lines:\n",
    "        feedback.append(f\"Uncovered lines: {sorted(set(missing_lines))}\")\n",
    "    if partial_lines:\n",
    "        feedback.append(f\"Lines with partial (branch) coverage: {sorted(set(partial_lines))}\")\n",
    "    feedback_text = \"\\n\".join(feedback) if feedback else \"No uncovered lines detected. Try to add more edge-case tests.\"\n",
    "    example = f\"\"\"\n",
    "    def some_test_cases(self):\n",
    "        self.assertFalse(methodname())\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are given a Python function (solution code) and coverage feedback from running our current unittest file.\n",
    "\n",
    "Goal:\n",
    "- Generate additional **Python unittest** test cases to improve code coverage, focusing on the uncovered or partially covered lines listed below.\n",
    "- Append tests that are **compatible** with the existing test file's style (standard unittest) and **do not alter the solution code**.\n",
    "\n",
    "Solution code:\n",
    "```python\n",
    "{solution_code}\n",
    "```\n",
    "Coverage feedback:\n",
    "{feedback_text}\n",
    "\n",
    "Instructions:\n",
    "- Write only valid Python unittest test cases that can be appended to the existing test class. For example:\n",
    "```python\n",
    "{example}\n",
    "```\n",
    "- Target the uncovered/partial lines specifically (exercise missing branches, edge inputs, error paths).\n",
    "- Do NOT include triple backticks in your output.\n",
    "- Return only the additional test methods like the example.\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ecced37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_up_to(5) => [2, 3]\n",
      "count_up_to(11) => [2, 3, 5, 7]\n",
      "count_up_to(0) => []\n",
      "count_up_to(20) => [2, 3, 5, 7, 11, 13, 17, 19]\n",
      "count_up_to(1) => []\n",
      "count_up_to(18) => [2, 3, 5, 7, 11, 13, 17]\n",
      "count_up_to(2) => []\n",
      "count_up_to(3) => [2]\n",
      "s='abcde', c='ae' -> ('bcd', False)\n",
      "s='abcdef', c='b' -> ('acdef', False)\n",
      "s='abcdedcba', c='ab' -> ('cdedc', True)\n",
      "s='racecar', c='c' -> ('raear', True)\n",
      "s='madam', c='' -> ('madam', True)\n",
      "s='hello', c='l' -> ('heo', False)\n",
      "s='a', c='a' -> ('', True)\n",
      "s='a', c='b' -> ('a', True)\n",
      "s='', c='xyz' -> ('', True)\n",
      "x_or_y(7, 34, 12) == 34\n",
      "x_or_y(15, 8, 5) == 5\n",
      "x_or_y(2, 100, 200) == 100\n",
      "x_or_y(4, 'apple', 'banana') == banana\n",
      "x_or_y(1, 'prime', 'not prime') == not prime\n",
      "x_or_y(11, 'found', 'missed') == found\n",
      "Average Coverage Rate: 73.35%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>entry_point</th>\n",
       "      <th>coverage</th>\n",
       "      <th>statements</th>\n",
       "      <th>missing</th>\n",
       "      <th>partial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>has_close_elements</td>\n",
       "      <td>80.00</td>\n",
       "      <td>10</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>1-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>separate_paren_groups</td>\n",
       "      <td>93.33</td>\n",
       "      <td>15</td>\n",
       "      <td>[1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>truncate_number</td>\n",
       "      <td>60.00</td>\n",
       "      <td>5</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>1-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>below_zero</td>\n",
       "      <td>85.71</td>\n",
       "      <td>7</td>\n",
       "      <td>[1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>mean_absolute_deviation</td>\n",
       "      <td>77.78</td>\n",
       "      <td>9</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>1-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>159</td>\n",
       "      <td>eat</td>\n",
       "      <td>80.00</td>\n",
       "      <td>5</td>\n",
       "      <td>[1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>160</td>\n",
       "      <td>do_algebra</td>\n",
       "      <td>87.50</td>\n",
       "      <td>8</td>\n",
       "      <td>[1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>161</td>\n",
       "      <td>solve</td>\n",
       "      <td>93.75</td>\n",
       "      <td>16</td>\n",
       "      <td>[1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>162</td>\n",
       "      <td>string_to_md5</td>\n",
       "      <td>57.14</td>\n",
       "      <td>7</td>\n",
       "      <td>[1, 2, 4]</td>\n",
       "      <td>1-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>163</td>\n",
       "      <td>generate_integers</td>\n",
       "      <td>87.50</td>\n",
       "      <td>8</td>\n",
       "      <td>[1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id              entry_point  coverage  statements    missing partial\n",
       "0      0       has_close_elements     80.00          10     [1, 3]     1-3\n",
       "1      1    separate_paren_groups     93.33          15        [1]       1\n",
       "2      2          truncate_number     60.00           5     [1, 3]     1-3\n",
       "3      3               below_zero     85.71           7        [1]       1\n",
       "4      4  mean_absolute_deviation     77.78           9     [1, 3]     1-3\n",
       "..   ...                      ...       ...         ...        ...     ...\n",
       "159  159                      eat     80.00           5        [1]       1\n",
       "160  160               do_algebra     87.50           8        [1]       1\n",
       "161  161                    solve     93.75          16        [1]       1\n",
       "162  162            string_to_md5     57.14           7  [1, 2, 4]     1-4\n",
       "163  163        generate_integers     87.50           8        [1]       1\n",
       "\n",
       "[164 rows x 6 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "for sol_file in sorted(GEN_SOL_DIR.glob(\"Solution_*.py\"), key=lambda p: int(re.search(r\"(\\d+)\", p.stem).group(1))):\n",
    "    idx = int(re.search(r\"(\\d+)\", sol_file.stem).group(1))\n",
    "    test_file = TESTS_DIR / f\"HumanEval_{idx}.py\"\n",
    "    if not test_file.exists():\n",
    "        print(f\"[SKIP] {idx:03d} missing test file\")\n",
    "        continue\n",
    "    sol_code = read_text(sol_file)\n",
    "    entry_point = infer_entry_point_from_solution(sol_code)\n",
    "    if not entry_point:\n",
    "        print(f\"[WARN] {idx:03d} could not infer entry point in {sol_file.name}\")\n",
    "        continue\n",
    "\n",
    "    sol_mod = load_solution_as_module(sol_file)\n",
    "    if not hasattr(sol_mod, entry_point):\n",
    "        print(f\"[WARN] {idx:03d} entry point {entry_point} not found in {sol_file.name}\")\n",
    "        continue\n",
    "\n",
    "    func_obj = getattr(sol_mod, entry_point)\n",
    "    test_mod = load_test_module(test_file, entry_point, func_obj)\n",
    "    suite = unittest.defaultTestLoader.loadTestsFromModule(test_mod)\n",
    "\n",
    "    cov, result = run_suite_under_coverage(sol_file, suite)\n",
    "    pct, statements, missing, partial = coverage_metrics_for_file(cov, str(sol_file))\n",
    "\n",
    "    # print(f\"\\n[{idx:03d}] entry: {entry_point}, coverage: {pct:5.1f}%, statement: {len(statements)}, missing: {len(missing)}, partial: {len(partial)}\")\n",
    "    if missing or partial:\n",
    "        prompt = make_augmentation_prompt(sol_code, missing, partial)\n",
    "        # print(\"Generated augmentation prompt:\\n\", prompt)\n",
    "\n",
    "    rows.append({\n",
    "        \"id\": idx,\n",
    "        \"entry_point\": entry_point,\n",
    "        \"coverage\": pct,\n",
    "        \"statements\": len(statements),\n",
    "        \"missing\": missing,\n",
    "        \"partial\": partial\n",
    "    })\n",
    "\n",
    "df_cov_aug = pd.DataFrame(rows).sort_values(\"id\").reset_index(drop=True)\n",
    "print(f\"Average Coverage Rate: {df_cov_aug['coverage'].mean():.2f}%\")\n",
    "df_cov_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae32fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cov_aug.to_csv(\"C:\\\\Users\\\\zhang\\\\Downloads\\\\cov_aug.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "34ee9036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Augment tests with Gemini based on coverage gaps, then re-measure coverage ===\n",
    "import os, re, io, sys, pathlib, types, importlib.util, builtins, unittest, textwrap, warnings\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "from coverage.exceptions import CoverageWarning\n",
    "\n",
    "# Reuse your existing helpers & imports already defined in your notebook:\n",
    "# - read_text, load_solution_as_module, infer_entry_point_from_solution,\n",
    "#   load_test_module, run_suite_under_coverage, coverage_metrics_for_file,\n",
    "#   make_augmentation_prompt, GEN_SOL_DIR, TESTS_DIR\n",
    "# Assumes: warnings.filterwarnings(\"ignore\", category=CoverageWarning) already set\n",
    "\n",
    "# --- Configure Gemini ---\n",
    "api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"GOOGLE_API_KEY not set. `export GOOGLE_API_KEY=...`\")\n",
    "genai.configure(api_key=api_key)\n",
    "gmodel = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "# ---------- small utilities for patching tests ----------\n",
    "\n",
    "def first_testcase_class_span(test_src: str):\n",
    "    \"\"\"\n",
    "    Find the first `class XXX(unittest.TestCase):` block and return:\n",
    "      (class_name, start_idx, end_idx, class_indent_str, inner_indent_str)\n",
    "    inner_indent_str is the indentation used for a method inside the class\n",
    "    (e.g., a tab or 4 spaces). If no method is found, defaults to 4 spaces.\n",
    "    \"\"\"\n",
    "    lines = test_src.splitlines(keepends=True)\n",
    "    class_pat = re.compile(r'^\\s*class\\s+([A-Za-z_]\\w*)\\s*\\(\\s*unittest\\.TestCase\\s*\\)\\s*:\\s*$', re.M)\n",
    "\n",
    "    cls_name = None\n",
    "    start = end = None\n",
    "    class_indent = \"\"\n",
    "    for i, ln in enumerate(lines):\n",
    "        m = class_pat.match(ln)\n",
    "        if m:\n",
    "            cls_name = m.group(1)\n",
    "            class_indent = re.match(r'^(\\s*)', ln).group(1)\n",
    "            # find end of class (next non-blank line with indent <= class_indent)\n",
    "            for j in range(i + 1, len(lines)):\n",
    "                ln2 = lines[j]\n",
    "                ind2 = len(ln2) - len(ln2.lstrip())\n",
    "                if ln2.strip() and ind2 <= len(class_indent):\n",
    "                    end = j\n",
    "                    break\n",
    "            if end is None:\n",
    "                end = len(lines)\n",
    "            start = i\n",
    "            break\n",
    "\n",
    "    if cls_name is None:\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    # Detect inner indent by finding the first method def in the class\n",
    "    inner_indent = \"    \"  # default 4 spaces\n",
    "    method_pat = re.compile(r'^\\s*def\\s+\\w+\\s*\\(', re.M)\n",
    "    for k in range(start + 1, end):\n",
    "        ln = lines[k]\n",
    "        if method_pat.match(ln):\n",
    "            line_indent = re.match(r'^(\\s*)', ln).group(1)\n",
    "            # inner indent is what's beyond the class indent\n",
    "            inner_indent = line_indent[len(class_indent):]\n",
    "            if not inner_indent:\n",
    "                inner_indent = \"    \"\n",
    "            break\n",
    "\n",
    "    return cls_name, start, end, class_indent, inner_indent\n",
    "\n",
    "def clean_llm_methods(raw_text: str, test_name_prefix: str = \"test_\") -> str:\n",
    "    \"\"\"\n",
    "    Robustly sanitize LLM output and return only *valid* unittest methods:\n",
    "\n",
    "    - remove code fences / markdown\n",
    "    - normalize newlines and tabs\n",
    "    - strip ANY junk before 'def test_...:' on a line (e.g., ': def ...', ', def ...')\n",
    "    - extract complete method blocks from each 'def test_*' to the line before the next 'def ' at the same indent\n",
    "    - dedent each method independently to fix body indentation\n",
    "    - join the cleaned methods with a single blank line\n",
    "\n",
    "    Returns an empty string if nothing usable is found.\n",
    "    \"\"\"\n",
    "    if not raw_text:\n",
    "        return \"\"\n",
    "\n",
    "    # 1) strip markdown/code fences & normalize whitespace\n",
    "    text = re.sub(r\"```(?:python)?\", \"\", raw_text)\n",
    "    text = re.sub(r\"```\", \"\", text)\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\").expandtabs(4)\n",
    "\n",
    "    # 2) line-level cleanup: if a line contains 'def test_', drop everything before the 'def'\n",
    "    cleaned_lines = []\n",
    "    for ln in text.split(\"\\n\"):\n",
    "        i = ln.find(\"def \")\n",
    "        j = ln.find(f\"def {test_name_prefix}\")\n",
    "        if j != -1:\n",
    "            ln = ln[j:]               # drop junk like ': ' or ', ' before def\n",
    "        elif i != -1 and re.search(rf\"def\\s+{re.escape(test_name_prefix)}\", ln[i:]):\n",
    "            ln = ln[i:]\n",
    "        cleaned_lines.append(ln)\n",
    "    text = \"\\n\".join(cleaned_lines)\n",
    "\n",
    "    # 3) extract method blocks that start with 'def test_*(' and end before the next top-level 'def '\n",
    "    methods = []\n",
    "    lines = text.split(\"\\n\")\n",
    "    n = len(lines)\n",
    "    i = 0\n",
    "    def_line_re = re.compile(rf'^\\s*def\\s+{re.escape(test_name_prefix)}[\\w]*\\s*\\(')\n",
    "    any_def_re  = re.compile(r'^\\s*def\\s+\\w+\\s*\\(')\n",
    "\n",
    "    while i < n:\n",
    "        if def_line_re.match(lines[i]):\n",
    "            # capture block\n",
    "            start = i\n",
    "            start_indent = len(lines[i]) - len(lines[i].lstrip())\n",
    "            i += 1\n",
    "            while i < n:\n",
    "                # next method at same or smaller indent ends this block\n",
    "                if any_def_re.match(lines[i]) and (len(lines[i]) - len(lines[i].lstrip())) <= start_indent:\n",
    "                    break\n",
    "                i += 1\n",
    "            block = \"\\n\".join(lines[start:i])\n",
    "            # dedent and trim trailing whitespace to normalize indentation\n",
    "            block = textwrap.dedent(block).strip(\"\\n\")\n",
    "            # ensure a colon at end of signature line\n",
    "            if not block.split(\"\\n\", 1)[0].rstrip().endswith(\":\"):\n",
    "                head, *rest = block.split(\"\\n\")\n",
    "                block = head.rstrip() + \":\\n\" + (\"\\n\".join(rest) if rest else \"    pass\")\n",
    "            methods.append(block)\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    return (\"\\n\\n\".join(methods)).strip()\n",
    "\n",
    "def indent_into_class(methods_block: str, class_indent: str, inner_indent: str) -> str:\n",
    "    \"\"\"\n",
    "    Dedent the given test methods and re-indent so they sit inside a class\n",
    "    at indentation: class_indent + inner_indent.\n",
    "    Ensures trailing newline.\n",
    "    \"\"\"\n",
    "    if not methods_block:\n",
    "        return \"\"\n",
    "    # Normalize + dedent (preserve relative indentation)\n",
    "    ded = textwrap.dedent(methods_block.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")).strip(\"\\n\")\n",
    "    lines = ded.split(\"\\n\")\n",
    "    prefix = class_indent + inner_indent\n",
    "    out_lines = []\n",
    "    for ln in lines:\n",
    "        if ln.strip():\n",
    "            out_lines.append(prefix + ln)\n",
    "        else:\n",
    "            out_lines.append(\"\")  # keep blank lines\n",
    "    out = \"\\n\".join(out_lines)\n",
    "    if not out.endswith(\"\\n\"):\n",
    "        out += \"\\n\"\n",
    "    return out\n",
    "\n",
    "def append_methods_into_test_file(test_path: pathlib.Path, methods_block: str, *, force_spaces: bool = False) -> bool:\n",
    "    \"\"\"\n",
    "    Append methods_block inside the first unittest.TestCase class.\n",
    "    - If no class exists, scaffold one.\n",
    "    - If force_spaces=True, class/inner indents are converted to spaces equivalents.\n",
    "\n",
    "    Returns True if the file was modified (methods added), False if methods_block is empty.\n",
    "    \"\"\"\n",
    "    methods_block = (methods_block or \"\").strip()\n",
    "    if not methods_block:\n",
    "        return False\n",
    "\n",
    "    src = read_text(test_path)\n",
    "    cls_name, start, end, class_indent, inner_indent = first_testcase_class_span(src)\n",
    "\n",
    "    # If requested, convert indent tokens to pure spaces (fallback mode)\n",
    "    if force_spaces:\n",
    "        # Convert detected indents to width-based spaces\n",
    "        class_spaces = \" \" * len(class_indent.expandtabs(4))\n",
    "        inner_spaces = \" \" * max(4, len(inner_indent.expandtabs(4)) or 4)\n",
    "        class_indent, inner_indent = class_spaces, inner_spaces\n",
    "\n",
    "    if cls_name is None:\n",
    "        # No TestCase class found: scaffold one and insert methods inside it\n",
    "        body = indent_into_class(methods_block, class_indent=\"\", inner_indent=\"    \")\n",
    "        scaffold = (\n",
    "            \"\\n\\nimport unittest\\n\\n\"\n",
    "            \"class GeneratedAugmentedTests(unittest.TestCase):\\n\"\n",
    "            f\"{body}\"\n",
    "        )\n",
    "        test_path.write_text(src + scaffold, encoding=\"utf-8\")\n",
    "        return True\n",
    "\n",
    "    # Insert inside the first class, right before its end\n",
    "    lines = src.splitlines(keepends=True)\n",
    "    insert_block = indent_into_class(methods_block, class_indent, inner_indent)\n",
    "\n",
    "    # Ensure there's a blank line before insertion if needed\n",
    "    if end > 0 and lines[end-1].strip():\n",
    "        insert_block = \"\\n\" + insert_block\n",
    "\n",
    "    lines[end:end] = [insert_block]\n",
    "    test_path.write_text(\"\".join(lines), encoding=\"utf-8\")\n",
    "    return True\n",
    "\n",
    "def try_load_tests(test_path: pathlib.Path, entry_point: str, func_obj):\n",
    "    \"\"\"\n",
    "    Try importing the test file with function injection. Return (ok: bool, module_or_err: Any).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tmod = load_test_module(test_path, entry_point, func_obj)\n",
    "        return True, tmod\n",
    "    except Exception as e:\n",
    "        return False, e\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7d497724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.0 [1, 3, 31, 34, 35, 40, 43, 44, 45, 48] [1, 3] 1-3\n",
      "93.33 [1, 28, 30, 31, 32, 35, 37, 40, 41, 42, 43, 47, 49, 51, 61] [1] 1\n",
      "  [REVERT] Added methods caused import error: invalid syntax (HumanEval_2.py, line 74)\n",
      "count_up_to(5) => [2, 3]\n",
      "count_up_to(11) => [2, 3, 5, 7]\n",
      "count_up_to(0) => []\n",
      "count_up_to(20) => [2, 3, 5, 7, 11, 13, 17, 19]\n",
      "count_up_to(1) => []\n",
      "count_up_to(18) => [2, 3, 5, 7, 11, 13, 17]\n",
      "count_up_to(2) => []\n",
      "count_up_to(3) => [2]\n",
      "count_up_to(5) => [2, 3]\n",
      "count_up_to(11) => [2, 3, 5, 7]\n",
      "count_up_to(0) => []\n",
      "count_up_to(20) => [2, 3, 5, 7, 11, 13, 17, 19]\n",
      "count_up_to(1) => []\n",
      "count_up_to(18) => [2, 3, 5, 7, 11, 13, 17]\n",
      "count_up_to(2) => []\n",
      "count_up_to(3) => [2]\n",
      "  [REVERT] Added methods caused import error: invalid syntax (HumanEval_103.py, line 60)\n",
      "s='abcde', c='ae' -> ('bcd', False)\n",
      "s='abcdef', c='b' -> ('acdef', False)\n",
      "s='abcdedcba', c='ab' -> ('cdedc', True)\n",
      "s='racecar', c='c' -> ('raear', True)\n",
      "s='madam', c='' -> ('madam', True)\n",
      "s='hello', c='l' -> ('heo', False)\n",
      "s='a', c='a' -> ('', True)\n",
      "s='a', c='b' -> ('a', True)\n",
      "s='', c='xyz' -> ('', True)\n",
      "  [REVERT] Added methods caused import error: invalid syntax (HumanEval_123.py, line 83)\n",
      "x_or_y(7, 34, 12) == 34\n",
      "x_or_y(15, 8, 5) == 5\n",
      "x_or_y(2, 100, 200) == 100\n",
      "x_or_y(4, 'apple', 'banana') == banana\n",
      "x_or_y(1, 'prime', 'not prime') == not prime\n",
      "x_or_y(11, 'found', 'missed') == found\n",
      "x_or_y(7, 34, 12) == 34\n",
      "x_or_y(15, 8, 5) == 5\n",
      "x_or_y(2, 100, 200) == 100\n",
      "x_or_y(4, 'apple', 'banana') == banana\n",
      "x_or_y(1, 'prime', 'not prime') == not prime\n",
      "x_or_y(11, 'found', 'missed') == found\n",
      "  [REVERT] Added methods caused import error: invalid syntax (HumanEval_156.py, line 95)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>entry_point</th>\n",
       "      <th>statements:</th>\n",
       "      <th>missing</th>\n",
       "      <th>partial</th>\n",
       "      <th>coverage_current</th>\n",
       "      <th>coverage_before</th>\n",
       "      <th>added_methods</th>\n",
       "      <th>coverage_after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>truncate_number</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>60.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>below_zero</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>85.71</td>\n",
       "      <td>85.71</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>mean_absolute_deviation</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>77.78</td>\n",
       "      <td>77.78</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>intersperse</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>85.71</td>\n",
       "      <td>85.71</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>parse_nested_parens</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92.86</td>\n",
       "      <td>92.86</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>159</td>\n",
       "      <td>eat</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.00</td>\n",
       "      <td>80.00</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>160</td>\n",
       "      <td>do_algebra</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87.50</td>\n",
       "      <td>87.50</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>161</td>\n",
       "      <td>solve</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>93.75</td>\n",
       "      <td>93.75</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>162</td>\n",
       "      <td>string_to_md5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>57.14</td>\n",
       "      <td>57.14</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>163</td>\n",
       "      <td>generate_integers</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87.50</td>\n",
       "      <td>87.50</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id              entry_point  statements:  missing  partial  \\\n",
       "0      2          truncate_number          5.0      2.0      3.0   \n",
       "1      3               below_zero          7.0      1.0      1.0   \n",
       "2      4  mean_absolute_deviation          9.0      2.0      3.0   \n",
       "3      5              intersperse          7.0      1.0      1.0   \n",
       "4      6      parse_nested_parens         14.0      1.0      1.0   \n",
       "..   ...                      ...          ...      ...      ...   \n",
       "157  159                      eat          5.0      1.0      1.0   \n",
       "158  160               do_algebra          8.0      1.0      1.0   \n",
       "159  161                    solve         16.0      1.0      1.0   \n",
       "160  162            string_to_md5          7.0      3.0      3.0   \n",
       "161  163        generate_integers          8.0      1.0      1.0   \n",
       "\n",
       "     coverage_current  coverage_before  added_methods  coverage_after  \n",
       "0               60.00            60.00              0             NaN  \n",
       "1               85.71            85.71              5             NaN  \n",
       "2               77.78            77.78              3             NaN  \n",
       "3               85.71            85.71              7             NaN  \n",
       "4               92.86            92.86              4             NaN  \n",
       "..                ...              ...            ...             ...  \n",
       "157             80.00            80.00              4             NaN  \n",
       "158             87.50            87.50              7             NaN  \n",
       "159             93.75            93.75             15             NaN  \n",
       "160             57.14            57.14              6             NaN  \n",
       "161             87.50            87.50              2             NaN  \n",
       "\n",
       "[162 rows x 9 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "for sol_file in sorted(GEN_SOL_DIR.glob(\"Solution_*.py\"), key=lambda p: int(re.search(r\"(\\d+)\", p.stem).group(1))):\n",
    "    idx = int(re.search(r\"(\\d+)\", sol_file.stem).group(1))\n",
    "    test_file = TESTS_DIR / f\"HumanEval_{idx}.py\"\n",
    "    if not test_file.exists():\n",
    "        print(f\"[SKIP] {idx:03d} missing test file: {test_file.name}\")\n",
    "        continue\n",
    "\n",
    "    solution_code = read_text(sol_file)\n",
    "    entry_point = infer_entry_point_from_solution(solution_code)\n",
    "    if not entry_point:\n",
    "        print(f\"[WARN] {idx:03d} could not infer entry point in {sol_file.name}\")\n",
    "        rows.append({\"id\": idx, \"entry_point\": None, \"coverage_before\": 0.0, \"coverage_after\": 0.0, \"added_methods\": 0})\n",
    "        continue\n",
    "\n",
    "    # Load solution and baseline coverage\n",
    "    sol_mod = load_solution_as_module(sol_file)\n",
    "    if not hasattr(sol_mod, entry_point):\n",
    "        print(f\"[WARN] {idx:03d} entry point '{entry_point}' not found in solution.\")\n",
    "        rows.append({\"id\": idx, \"entry_point\": entry_point, \"coverage_before\": 0.0, \"coverage_after\": 0.0, \"added_methods\": 0})\n",
    "        continue\n",
    "\n",
    "    func = getattr(sol_mod, entry_point)\n",
    "    test_mod = load_test_module(test_file, entry_point, func)\n",
    "    suite = unittest.defaultTestLoader.loadTestsFromModule(test_mod)\n",
    "\n",
    "    cov_before, _ = run_suite_under_coverage(sol_file, suite)\n",
    "    pct_before, statements, missing, partial = coverage_metrics_for_file(cov_before, str(sol_file))\n",
    "\n",
    "    if idx <= 1:\n",
    "        print(pct_before, statements, missing, partial)\n",
    "        continue\n",
    "    # print(f\"\\n[{idx:03d}] entry={entry_point:25s} coverage_before={pct_before:5.1f}%  missing={len(missing)}  partial={len(partial)}\")\n",
    "\n",
    "    # Build prompt and print it (as requested)\n",
    "    prompt_text = make_augmentation_prompt(solution_code, missing, partial)\n",
    "    # print(\"\\n--- Augmentation prompt ---\\n\")\n",
    "    # print(prompt_text)\n",
    "    # print(\"\\n--- End prompt ---\\n\")\n",
    "\n",
    "    # If nothing missing/partial, skip augmentation\n",
    "    if not missing and not partial:\n",
    "        rows.append({\n",
    "            \"id\": idx, \"entry_point\": entry_point,\n",
    "            \"coverage_before\": pct_before, \"coverage_after\": pct_before,\n",
    "            \"added_methods\": 0\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # Ask Gemini for additional test methods\n",
    "    resp = gmodel.generate_content(prompt_text)\n",
    "    add_methods_raw = (getattr(resp, \"text\", None) or \"\").strip()\n",
    "    add_methods_clean = clean_llm_methods(add_methods_raw)\n",
    "    # print(\"clean method:\\n\", add_methods_clean or \"[no methods generated]\")\n",
    "\n",
    "    # Keep a backup\n",
    "    original_test_src = test_file.read_text(encoding=\"utf-8\")\n",
    "\n",
    "    added = 0\n",
    "    if add_methods_clean:\n",
    "        modified = append_methods_into_test_file(test_file, add_methods_clean)\n",
    "        if modified:\n",
    "            # Validate by importing tests; if broken, revert\n",
    "            ok, mod_or_err = try_load_tests(test_file, entry_point, func)\n",
    "            if not ok:\n",
    "                print(f\"  [REVERT] Added methods caused import error: {mod_or_err}\")\n",
    "                test_file.write_text(original_test_src, encoding=\"utf-8\")\n",
    "            else:\n",
    "                added = len(re.findall(r'^\\s*def\\s+test_', add_methods_clean, flags=re.M))\n",
    "\n",
    "    # Re-run coverage after augmentation (or after revert)\n",
    "    sol_mod = load_solution_as_module(sol_file)  # ensure fresh module\n",
    "    func = getattr(sol_mod, entry_point, func)\n",
    "    test_mod = load_test_module(test_file, entry_point, func)\n",
    "    suite = unittest.defaultTestLoader.loadTestsFromModule(test_mod)\n",
    "    cov_after, result_after = run_suite_under_coverage(sol_file, suite)\n",
    "    pctNew, statementsNew, missingNew, partialNew = coverage_metrics_for_file(cov_after, str(sol_file))\n",
    "\n",
    "    # print(f\"  coverage_after={pctNew:5.1f}%  (+{pctNew - pct_before:+.1f} pp)  added_methods={added}\")\n",
    "\n",
    "    rows.append({\n",
    "        \"id\": idx,\n",
    "        \"entry_point\": entry_point,\n",
    "        \"statements:\": len(statementsNew),\n",
    "        \"missing\": len(missingNew),\n",
    "        \"partial\": len(partialNew),\n",
    "        \"coverage_current\": pctNew,\n",
    "        \"coverage_before\": pct_before,\n",
    "        \"added_methods\": added,\n",
    "    })\n",
    "    # break  # DEBUG\n",
    "\n",
    "df_aug_cov2 = pd.DataFrame(rows).sort_values(\"id\").reset_index(drop=True)\n",
    "df_aug_cov2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
